{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sla and sss subplots](../images/earthaccess_subplots.png \"Image showing two maps of the Western Tropical Atlantic, both with data for the date 2020-09-23. On the left, a map of sea surface height anomaly, and on the right, a map of sea surface salinity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data access via earthaccess library and vizualization with cartopy\n",
    "\n",
    "<!-- In this Notebook, we will access data via the earthaccess python library. Then, we will do some quick visualization to find out the data of interest, to then make informational plots and, if desired, download data.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Within this notebook, we will cover:\n",
    "1. How to search data via Earthdata Search web application\n",
    "2. How to access NASA Earth Science data via `earthaccess`\n",
    "3. How to subset, set attributes and modify coordinates for `xarray` Datasets\n",
    "4. How to visualize data with `hvplot` and `cartopy`\n",
    "5. How to download data\n",
    "\n",
    "<!-- Downloading data via Graphical User Interfaces (GUI) can be great for small datasets in isolated instances, but it has a few disadvantages, such as\n",
    "* it can be tedious - if you have to get data multiple times, repeatedly doing the same task is not a lot of fun, and this time could be better spent\n",
    "* it is more prone to human error - if you do many times the same task, odds are at some point you will make some mistake\n",
    "* storage issues - Sometimes it is hard to subset the data via GUI and you're stuck downloading large files when you needed just a small portion of them\n",
    "* it is not best science reproducibility practice - you can have the source of the data in your documents, but if you can have the actual data access method within the code, that makes it much easier for the work to be reproduced and for other researchers to build upon it\n",
    "\n",
    "Accessing data programatically, via application programming interface (API), is pretty advantageous when you consider those issues. In this notebook we will present an example of a workflow with access to data via `earthaccess`, a python library for accessing NASA Earth Science data.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "<!-- In this notebook, we will access some data with the `xarray` library and plot some maps with the `matplotlib` and `cartopy` python libraries. -->\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Necessary | Data and metadata structure|\n",
    "| [netCDF](https://foundations.projectpythia.org/core/data-formats/netcdf-cf.html) | Helpful | Data and metadata structure|\n",
    "| [Understanding of matplotlib](https://foundations.projectpythia.org/core/matplotlib/matplotlib-basics.html) | Helpful | Familiarity with plots |\n",
    "| [Intro to Cartopy](https://foundations.projectpythia.org/core/cartopy/cartopy.html) | Helpful | Familiarity with maps|\n",
    "\n",
    "- **Time to learn**: 30 minutes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import earthaccess \n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cartopy import crs as ccrs, feature as cfeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea surface height anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for some sea surface height anomaly data for the Western Tropical Atlantic. In order to access it, we will use the `earthaccess` python library. It is used to make it easier for the user to find, stream and download NASA Earth Science data. More information about this library can be found in their [documentation page](https://earthaccess.readthedocs.io/en/latest/) and [Github repository](https://github.com/nsidc/earthaccess). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering for an Earthdata account and authenticating\n",
    "\n",
    "Before searching for data, it is necessary to register for an Earthdata login profile, which can be done easily and quickly [this way](https://urs.earthdata.nasa.gov/documentation/for_users/how_to_register).\n",
    "\n",
    "After registering, earthaccess has to authenticate you as a user. There are a few ways to do that, as stated [here](https://earthaccess.readthedocs.io/en/latest/user-reference/api/api/#earthaccess.api.login). The easiest way of doing is just executing `auth = earthaccess.login()` in your jupyter notebook, and that would prompt for input of the username and password for the user.\n",
    "\n",
    "For the purposes of this demonstration we have environment variables that will be used for authentication. So you just need to execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login(strategy=\"environmnent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After it is authenticated, we are ready to start our search for data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching and accessing the data\n",
    "\n",
    "`earthaccess` allows us to look for datasets (called `DataCollections`) and specific data files (called `DataGranules`). To look for them, we need some criteria in order to perform the search. Here we will use \n",
    "- a shortname, which is a dataset identifier;\n",
    "- a temporal window: we want data between those dates; and\n",
    "- a bounding box: we want data within that area.\n",
    "\n",
    "One good way to get a better understanding of what to look for is visiting the [Earthdata Search website](https://search.earthdata.nasa.gov/search). There you can search by keywords and select filters to see which data could be helpful in your research. Once you find a dataset (*Collection*) that interests you, you click on its name. Below we show an example in which the \"MEaSUREs Gridded Sea Surface Height Anomalies Version 2205\" is the dataset of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Find Collection](../images/earthdata_find_collection.png \"Image from the Earthdata Search website showing a list of data collections and the 'MEaSUREsn Gridded Sea Surface Height Anomalies Version 2205' collection highlighted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking on the dataset name, you click on `Collection Details`, which will send you to a page that has more information about the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Collection details](../images/earthdata_collection_details.png \"Image from the Earthdata Search website showing a list of granules from the 'MEaSUREsn Gridded Sea Surface Height Anomalies Version 2205' collection, and indicating to click on the 'Collection Details' button on the top right of the screen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to read the details about the dataset, to make sure that's what would help you in your work. If so, copy the name, usually all in caps, that is shown inside a light gray box on the top of the page. That is the identifier for that specific dataset. That's the main information we'll need to provide `earthaccess` library to search for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Shortname](../images/earthdata_shortname.png \"Image from the Earthdata Search website showing the details of the 'MEaSUREs Gridded Sea Surface Height Anomalies Version 2205' data collection, indicating the name 'SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205' as the short name for this particular dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an identifier for the dataset we want to access, let's use `earthaccess` to find sea surface height anomaly data in September and October, 2020, in the Western Tropical Atlantic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the bounding box for the Western Tropical Atlantic\n",
    "lonmin, latmin, lonmax, latmax = -70, -5, -45, 20\n",
    "\n",
    "# https://search.earthdata.nasa.gov/search/granules/collection-details?p=C2270392799-POCLOUD&pg%5B0%5D%5Bv%5D=f&pg%5B0%5D%5Bgsk%5D=-start_date&as%5Bscience_keywords%5D%5B0%5D=Oceans%3ASea%20Surface%20Topography%3ASea%20Level%3ASea%20Level%20Anomaly&tl=1718059227%213%21%21&fsm0=Sea%20Surface%20Topography&fst0=Oceans&fst1=Oceans&fsm1=Sea%20Surface%20Topography&fs11=Sea%20Level&fs21=Sea%20Level%20Anomaly&fpb0=Space-based%20Platforms&long=-0.0703125\n",
    "sla_shortname = \"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\"\n",
    "\n",
    "sla_results = earthaccess.search_data(\n",
    "    short_name=sla_shortname,\n",
    "    cloud_hosted=True,\n",
    "    temporal=(\"2020-09-01\", \"2020-10-30\"),\n",
    "    bounding_box=(lonmin, latmin, lonmax, latmax)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-info\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Info</p>\n",
    "    Note: not all these arguments are necessary, but the more arguments you use, the more refined the search will be. More info \n",
    "<a href=\"https://earthaccess.readthedocs.io/en/latest/user-reference/api/api/#earthaccess.api.search_data\">here.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `search_data` returns a list with the *Granules* found. Here we can think of Granules as files, or time-steps. Great, we found some that match our criteria. Now, let's take a look at one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sla_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have all the metadata information about this first Granule. You can also check all the Granules from the list, if you'd like to make sure they are all of interest to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sla_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we found some results that interest us. Now let's access them!\n",
    "\n",
    "Two possibly important limiting factors for research are limited computing capacity and limited data storage. Even if data storage is not very limited, unecessarily working with large files is not best practice. So here we'll not download the data; we'll stream it instead, using [xarray](https://docs.xarray.dev/en/stable/). Through xarray we can take a look at the data, subset, perform some analyses and, only if we are certain that we want to look further into it, download the data.\n",
    "\n",
    "So let's start by opening the data Granules as a dataset with xarray. This can take some time, depending on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "sla_ds = xr.open_mfdataset(earthaccess.open(sla_results))\n",
    "sla_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting the data, adjusting the coordinates and assigning attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the loaded dataset, we notice a few things:\n",
    "   * The search yielded 12 granules, so we have 12 time-steps in the resulting dataset\n",
    "   * The `Latitude` and `Longitude` span a much greater area than requested in the search. The dataset loaded contains the area that we requested, plus much more.\n",
    "   * we have a few data variables besides `SLA` (Sea Level Anomaly Estimate)\n",
    "   * the `Longitude` coordinate goes from 0 to 360 instead of from -180 to 180.\n",
    "   * there are multiple attributes. Those are very important, since they contain dataset metadata information.\n",
    "   \n",
    "That being said, we need to do a few things to make this dataset ready for analysis, visualization, and/or download.\n",
    "\n",
    "The first thing to do is to adjust the `Longitude` coordinates and sort the data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Longitude from 0-360 to -180-180\n",
    "sla_ds.coords['Longitude'] = (sla_ds.coords['Longitude'] + 180) % 360 - 180\n",
    "sla_ds = sla_ds.sortby(sla_ds.Longitude)\n",
    "sla_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the dataset has Longitude values between -180 and 180. Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to subset the data - we don't need SLA for the entire globe, just for the Western Tropical Atlantic, so there's no need to be messing with large files. To subset the data, we leverage the power of slicing in xarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sla_subset = sla_ds['SLA'].sel(Latitude=slice(latmin, latmax), Longitude=slice(lonmin,lonmax))\n",
    "sla_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better, but in order for this to be the main object of analysis and for us to be able to delete the original dataset (spanning the entire global ocean), we need to make sure it has metadata information. In order to do that, we gather some attributes from the original dataset, remove the ones that don't make sense after subsetting, and assign these attributes to the sla_subset DataArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes from the original Dataset\n",
    "sla_attrs = sla_ds.attrs\n",
    "del sla_ds\n",
    "\n",
    "# remove some attributes\n",
    "attrs_to_be_removed = ['geospatial_lat_min', \n",
    "                       'geospatial_lat_max', \n",
    "                       'geospatial_lon_min', \n",
    "                       'geospatial_lon_max', \n",
    "                       'time_coverage_start', \n",
    "                       'time_coverage_end']\n",
    "for attr in attrs_to_be_removed:\n",
    "    del sla_attrs[attr]\n",
    "    \n",
    "# assign attributes to DataArray    \n",
    "sla_subset = sla_subset.assign_attrs(sla_attrs)\n",
    "\n",
    "# check that the new attributes are there\n",
    "sla_subset.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that we have data and metadata. Now, let's take a quick look at the data for the few time-steps that we have loaded and see if they show any interesting features for us to investigate further. We can do that easily and interactively with hvplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sla_subset.hvplot.image(x='Longitude', y='Latitude', aspect=\"equal\", cmap='RdBu_r', clim=(-0.4, 0.4), title=\"Sea Level Anomaly Estimate (m)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspection, we decide that the data for Sept 23rd, 2020 looks promising. So let's subset further, to have just this time-step in our DataArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_selection = '2020-09-23' \n",
    "sla_subset_plot = sla_subset.sel(Time=date_selection)\n",
    "del sla_subset\n",
    "sla_subset_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the subsetted DataArray has all the attributes that we have assigned from the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sea Surface height anomaly visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a more ellaborate visualization of the data to analyze the features better. For that, we will use the library `cartopy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "fig = plt.figure(figsize=(11, 8.5))\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "# add features to map\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='black')\n",
    "gl = ax.gridlines(\n",
    "    draw_labels=True, linewidth=2, color='gray', alpha=0.5, linestyle='--'\n",
    ")\n",
    "ax.set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())\n",
    "\n",
    "# plot data\n",
    "levs = np.linspace(0,0.5,6)\n",
    "fmt = '%1.1f'\n",
    "sla_subset_plot.plot(vmin=-0.5, vmax=0.5, cmap ='RdBu_r', transform=ccrs.PlateCarree())\n",
    "cs = sla_subset_plot.squeeze().plot.contour(levels=levs,colors='k')\n",
    "ax.clabel(cs, levs, fmt=fmt, inline=True, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colorbar centered in zero helps us understand the data better. The main features are in the eastern part, where close to the coast we can see strong gradients, but altimetry data too close to the coast might have some issues. So we focus on the features further from the coast, and the main ones we see are circular contour lines between 5°N and 10°N, and 45°W and 55°W. Some knowledge of the ocean circulation patterns in the area suggest that these features are associated with the North Brazil Current retroflection and a North Brazil Current Ring. Can we see any signature of that in salinity? Let's check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sea surface salinity data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching and accessing the data\n",
    "\n",
    "Similarly to what was done for sea surface height anomaly, we'll visit the  [Earthdata Search website](https://search.earthdata.nasa.gov/search) to see what salinity data is available for the region and the time-period we're interested. After some search, we find that the dataset with the shortname \"SMAP_RSS_L3_SSS_SMI_8DAY-RUNNINGMEAN_V5\" seems applicable, even though it's an 8-day mean. Let's set the same bounding box, the time span for around the same day we analyzed for sea surface height anomaly, and see what data is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://search.earthdata.nasa.gov/search/granules/collection-details?p=C2208425700-POCLOUD&pg%5B0%5D%5Bv%5D=f&pg%5B0%5D%5Bqt%5D=2021-09-01%2C2021-11-30&pg%5B0%5D%5Bgsk%5D=-start_date&tl=1718241606.658%213%21%21\n",
    "sss_dataname=\"SMAP_RSS_L3_SSS_SMI_8DAY-RUNNINGMEAN_V5\"\n",
    "\n",
    "sss_results = earthaccess.search_data(\n",
    "    short_name=sss_dataname,\n",
    "    cloud_hosted=True,\n",
    "    temporal=(\"2020-09-23\",\"2020-09-24\"), # considering salt_results = salt_results[::3], the second time-step is the best for showing a ring and comparing to altimetry\n",
    "    bounding_box=(lonmin, latmin, lonmax, latmax)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there were 9 granules for the time span between 2020-09-23 and 2020-09-24, which seems excessive. Let's take a look at some of the granules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss_results[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the EndingDateTime and BeginningDateTime for each granule, we conclude that `earthaccess` found all the granules in which the date 2020-09-23 and/or 2020-09-24 were used to calculate the 8-day mean, and that's why there are so many granules. \n",
    "\n",
    "Let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sss_ds = xr.open_mfdataset(earthaccess.open(sss_results))\n",
    "sss_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset we see many of the same issues we saw in the sea surface height anomaly dataset:\n",
    "* large area, much bigger than the bouding box\n",
    "* other data variables besides sea surface salinity\n",
    "* longitude between 0 and 360 instead of -180 and 180\n",
    "* multiple attributes - that's a good thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting the data, adjusting the coordinates and assigning attributes\n",
    "\n",
    "Now we need to adjust this dataset, very similarly to what we did for the sea surface height anomaly one. First, we convert the longitude coordinates and sort the data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss_ds.coords['lon'] = (sss_ds.coords['lon'] + 180) % 360 - 180\n",
    "sss_ds = sss_ds.sortby(sss_ds.lon)\n",
    "sss_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we subset just for the variable we want, for the area within the bounding box, and make sure to assign most attributes from the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geographically subset\n",
    "sss_subset = sss_ds['sss_smap_40km'].sel(lat=slice(latmin, latmax), lon=slice(lonmin,lonmax))\n",
    "\n",
    "# get attributes from original dataset\n",
    "sss_attrs = sss_ds.attrs\n",
    "del sss_ds\n",
    "\n",
    "# remove attributes that don't apply\n",
    "attrs_to_be_removed = ['center_day_of_observation',\n",
    "                      'first_orbit',\n",
    "                      'last_orbit',\n",
    "                      'geospatial_lat_min', \n",
    "                      'geospatial_lat_max', \n",
    "                      'geospatial_lon_min', \n",
    "                      'geospatial_lon_max', \n",
    "                      'time_coverage_start', \n",
    "                      'time_coverage_end']\n",
    "for attr in attrs_to_be_removed:\n",
    "    del sss_attrs[attr]\n",
    "    \n",
    "sss_subset = sss_subset.assign_attrs(sss_attrs)\n",
    "sss_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to choose a time-step. If we look at the coordinates \"time\", we'll notice that the days in it are the center days of the running mean. So we'll just pick the same day used for the sea surface height anomaly data, '2020-09-23':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_selection = '2020-09-23'\n",
    "sss_subset_plot = sss_subset.sel(time=date_selection)\n",
    "del sss_subset\n",
    "\n",
    "sss_subset_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sea surface salinity visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we got a DataArray with just one time-step; now let's plot a map of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the figure\n",
    "fig = plt.figure(figsize=(23, 8.5))\n",
    "\n",
    "# add features to map\n",
    "ax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='black')\n",
    "gl = ax.gridlines(\n",
    "    draw_labels=True, linewidth=2, color='gray', alpha=0.5, linestyle='--'\n",
    ")\n",
    "ax.set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())\n",
    "\n",
    "# plot data\n",
    "levs_sss = np.linspace(5,37,12)\n",
    "fmt_sss = '%1.0f'\n",
    "sss_subset_plot.plot(vmin=5, vmax=37, transform=ccrs.PlateCarree())\n",
    "cs = sss_subset_plot.squeeze().plot.contour(levels=levs_sss,colors='k')\n",
    "ax.clabel(cs, levs_sss, fmt=fmt_sss, inline=True, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sea surface salinity map seems good, but if we put the plots next to each other, we could have a better understanding of what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea surface height anomaly and sea surface salinity combined visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,\n",
    "                               subplot_kw = {'projection':ccrs.PlateCarree()},\n",
    "                               figsize=(25, 8.5))\n",
    "# add features to subplots\n",
    "for ax in (ax1, ax2):\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='black')\n",
    "    gl = ax.gridlines(\n",
    "        draw_labels=True, linewidth=2, color='gray', alpha=0.5, linestyle='--'\n",
    "    )\n",
    "    ax.set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())\n",
    "\n",
    "# plot sla\n",
    "levs_sla = np.linspace(0,0.5,6)\n",
    "fmt_sla = '%1.1f'\n",
    "sla_subset_plot.plot(ax= ax1,vmin=-0.5, vmax=0.5, cmap ='RdBu_r', transform=ccrs.PlateCarree())\n",
    "cs = sla_subset_plot.squeeze().plot.contour(ax= ax1,levels=levs_sla,colors='k')\n",
    "ax1.clabel(cs, levs_sla, fmt=fmt_sla, inline=True, fontsize=10)\n",
    "\n",
    "# plot sss\n",
    "levs_sss = np.linspace(5,37,10)\n",
    "fmt_sss = '%1.0f'\n",
    "sss_subset_plot.plot(ax= ax2,vmin=5, vmax=37, transform=ccrs.PlateCarree())\n",
    "cs = sss_subset_plot.squeeze().plot.contour(ax= ax2,levels=levs_sss,colors='k')\n",
    "ax1.clabel(cs, levs_sss, fmt=fmt_sss, inline=True, fontsize=10)\n",
    "plt.savefig(\"subplots.png\",dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great; with the maps side-by-side, it's easier to understand the features. In the salinity map, we can see the freshwater discharge from the Orinoco and Amazon Rivers, close to the coast, at about 61°W,9°N and -48°W,0°, respectively. In addition, we can see a somewhat circular fresher water feature between about 50°W-55°W and 5°N-10°N. The location of this feature coincides with the location of the high circular sea surface height anomaly contours, strenghtening our hypothesis that it's a North Brazil Current Ring transporting freshwater from the Amazon River to the Caribbean. To the right, one can see the higher salinity values in a concave configuration, coinciding with the location of other circular sea surface height anomaly contours, suggesting this is a signature of the North Brazil Current retroflection, bringing salty water from the Equatorial Atlantic. For more information on this process, see [this article](https://bg.copernicus.org/articles/19/2969/2022/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data\n",
    "\n",
    "Remember: we have done all this visualization without downloading one single data file! That's one of the best functionalities of xarray, the ability of looking at the data with \"no strings attached\", so you can only download once you know that dataset will work for you.\n",
    "\n",
    "Now that we've seen that the subset DataArrays have interesting features, we may want to download them to our local machines, so we can analyze them further. That's very easy to do with xarray, by saving it in netCDF format. Here we build some filenames to use to save the DataArrays, but of course we could use any filename.\n",
    "\n",
    "One very important thing to notice is that the saved files will contain the attributes we assigned from the original Dataset. This is necessary for reproducibility, so other people can build on your work, or even future you can have access to details of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition alert alert-warning\">\n",
    "    <p class=\"admonition-title\" style=\"font-weight:bold\">Warning</p>\n",
    "    Here we leave the saving commands commented, because if you continuously save files you can run out of storage. So be mindful when saving files!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss_filename = sss_subset_plot.attrs['standard_name'] + \".nc\"\n",
    "sla_filename = sla_subset_plot.attrs['standard_name'] + \".nc\"\n",
    "\n",
    "# saving files - be careful here so you won't run out of storage!\n",
    "# sss_subset_plot.to_netcdf(sss_filename)\n",
    "# sla_subset_plot.to_netcdf(sla_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook we were able to leverage the `earthaccess` library to access large datasets with `xarrray` and visualize them with `matplotlib` and `cartopy`. We learned the advantages of accessing data programatically, including promoting science reproducibility. We were able to subset data and then, only when we identified data that would be of interest, download it. That is specially advantageous for cases with limited available storage or limited computing capacity.\n",
    "\n",
    "\n",
    "### What's next?\n",
    "[Nasa Earthdata Search](https://search.earthdata.nasa.gov) has various types of data, in different formats and from different platforms. The user is certainly encouraged to play around with other types of data, to become more comfortable with this tool.\n",
    "\n",
    "The `earthaccess` library is a wrap around the NASA Earth Science APIs. With the knowledge from this notebook, the user may understand bettter the structure of metadata in APIs and feel more comfortable accessing data via APIs from other sources, such as [NCEI](https://www.ncei.noaa.gov/support/access-data-service-api-user-documentation) and [USGS](https://data.usgs.gov/datacatalog/api/docs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources and references\n",
    "\n",
    "* [Earthdata Search web application](https://search.earthdata.nasa.gov/)\n",
    "* [info on earthaccess](https://www.earthdata.nasa.gov/learn/blog/earthaccess)\n",
    "* [earthaccess Github repository](https://github.com/nsidc/earthaccess)\n",
    "* [earthaccess documentation page](https://earthaccess.readthedocs.io/en/latest/)\n",
    "* [Earthdata cloud clinic](https://nasa-openscapes.github.io/earthdata-cloud-cookbook/tutorials/Earthdata-cloud-clinic.html)\n",
    "* [USGS Science Data Catalog API Documentation](https://data.usgs.gov/datacatalog/api/docs#/Harvest/read_harvested_files_harvest_files_get)\n",
    "* [NCEI API user documentation](https://www.ncei.noaa.gov/support/access-data-service-api-user-documentation)\n",
    "* [Intro to cartopy](https://foundations.projectpythia.org/core/cartopy/cartopy.html)\n",
    "* [Matplotlib basics](https://foundations.projectpythia.org/core/matplotlib/matplotlib-basics.html)\n",
    "* [netCDF and CF: the basics](https://foundations.projectpythia.org/core/data-formats/netcdf-cf.html)\n",
    "* [Introduction to Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html)\n",
    "* [Article on North Brazil Current Rings](https://bg.copernicus.org/articles/19/2969/2022/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
