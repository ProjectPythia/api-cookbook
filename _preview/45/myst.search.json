{"version":"1","records":[{"hierarchy":{"lvl1":"API Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"API Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers the basics of retrieving and visualizing data using APIs with Python.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"API Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"API Cookbook","lvl2":"Motivation"},"content":"There are many ways to gather data. Science and research entities like NASA are constantly producing and collecting data. As a result, attempting to collect and display live data can be difficult since new data is always being added or modified.\n\nAn API is a method to query a data source over the internet to retrieve data from a remote source. APIs are useful tools for working with live and constantly updating data sources. However, the terminology and methods for retrieving and manipulating the data in Python can make APIs confusing.\n\nThis cookbook focuses on accessing and visualizing data from various geoscience related APIs. Over the course of the cookbook, we will show step-by-step tutorials on retrieving data from some public APIs, as well as creating informational and visually appealing graphics to communicate the data to a general audience.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"API Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"API Cookbook","lvl2":"Authors"},"content":"Cora Schneck, \n\nAna Krelling, \n\nAdam Deitsch, \n\nHannah Zafar","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"API Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"API Cookbook","lvl2":"Structure"},"content":"This cookbook will be broken up into two main sections: “Foundations” to cover the basics of working with and understanding APIs and “Example Workflows” for complete working examples.","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"API Foundations","lvl2":"Structure"},"type":"lvl3","url":"/#api-foundations","position":10},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"API Foundations","lvl2":"Structure"},"content":"API Foundations will cover the terminology of APIs and how to make use of the data retrieved from API in Python.","type":"content","url":"/#api-foundations","position":11},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"Example Workflows","lvl2":"Structure"},"type":"lvl3","url":"/#example-workflows","position":12},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"Example Workflows","lvl2":"Structure"},"content":"Example Workflows will cover complete example of working with various APIs. This includes how to retrieve and understand data returned from different sources and manipulate the data to produce useful and appealing plots.\n\nJPL Center for Near-Earth Orbit Studies (CNEOS) API: Visualize the location and total impact energy of fireballs and bolides on a world map using the CNEOS \n\nFireball API.\n\nEarthdata API: Access and visualize sea surface height and sea surface salinity data using the National Aeronautics and Space Administration’s \n\nearthaccess Python package.\n\nAir Quality System (AQS) API: Visualize and compare air quality and atmospheric chemistry concentrations over geographic areas by leveraging the Environmental Protection Agency’s \n\npyaqsapi Python package.\n\nWhiteface Mountain Cloud Water Request: Request access to recent and historical cloud water chemistry data from the \n\nLance Research Laboratory at the Atmospheric Sciences Research Center.","type":"content","url":"/#example-workflows","position":13},{"hierarchy":{"lvl1":"API Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":14},{"hierarchy":{"lvl1":"API Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":15},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":16},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":17},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":18},{"hierarchy":{"lvl1":"API Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/api-cookbook repository: git clone https://github.com/ProjectPythia/api-cookbook.git\n\nMove into the api-cookbook directorycd api-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate cookbook-example\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":19},{"hierarchy":{"lvl1":"API Basics"},"type":"lvl1","url":"/notebooks/api-foundations/api-basics","position":0},{"hierarchy":{"lvl1":"API Basics"},"content":"\nPhoto by \n\nFotis Fotopoulos on \n\nUnsplash\n\n","type":"content","url":"/notebooks/api-foundations/api-basics","position":1},{"hierarchy":{"lvl1":"API Basics"},"type":"lvl1","url":"/notebooks/api-foundations/api-basics#api-basics","position":2},{"hierarchy":{"lvl1":"API Basics"},"content":"\n\n\n\n","type":"content","url":"/notebooks/api-foundations/api-basics#api-basics","position":3},{"hierarchy":{"lvl1":"API Basics","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/api-foundations/api-basics#overview","position":4},{"hierarchy":{"lvl1":"API Basics","lvl2":"Overview"},"content":"This notebook will cover the terminology and steps needed to retrieve data from an API\n\nPrerequisites\n\nMotivation\n\nAPI Terminology (and Status Codes)\n\nImports\n\nExample: Weather API\n\nSummary\n\n","type":"content","url":"/notebooks/api-foundations/api-basics#overview","position":5},{"hierarchy":{"lvl1":"API Basics","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/api-foundations/api-basics#prerequisites","position":6},{"hierarchy":{"lvl1":"API Basics","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Pandas\n\nNecessary\n\nFamiliarity with working with dataframes\n\nIntro to Matplotlib\n\nHelpful\n\nPlotting on a data\n\n\n\n","type":"content","url":"/notebooks/api-foundations/api-basics#prerequisites","position":7},{"hierarchy":{"lvl1":"API Basics","lvl2":"Motivation"},"type":"lvl2","url":"/notebooks/api-foundations/api-basics#motivation","position":8},{"hierarchy":{"lvl1":"API Basics","lvl2":"Motivation"},"content":"\n\nThere are many ways to gather data. Science and research entities like NASA are constantly producing and collecting data. As a result, attempting to collect and display live data can be difficult since new data is always being added. An API is a method to query a data source for the most current data or retrieve data from a remote source. This notebook will cover the basics regarding API usage to improve ease of access to publicly available data.\n\n\n\n","type":"content","url":"/notebooks/api-foundations/api-basics#motivation","position":9},{"hierarchy":{"lvl1":"API Basics","lvl2":"API Terminology"},"type":"lvl2","url":"/notebooks/api-foundations/api-basics#api-terminology","position":10},{"hierarchy":{"lvl1":"API Basics","lvl2":"API Terminology"},"content":"API: Application Programming Interface which dictacts how code can communicate and access or update remote data through methods\n\nRequest: Code sends requests to an API to either retrieve or update data\n\nGET: A GET request retrieves data\n\nSET: A SET request updates data\n\nWhen working with public APIs, most methods will request data from an API (a GET request)","type":"content","url":"/notebooks/api-foundations/api-basics#api-terminology","position":11},{"hierarchy":{"lvl1":"API Basics","lvl3":"Understanding Status Codes","lvl2":"API Terminology"},"type":"lvl3","url":"/notebooks/api-foundations/api-basics#understanding-status-codes","position":12},{"hierarchy":{"lvl1":"API Basics","lvl3":"Understanding Status Codes","lvl2":"API Terminology"},"content":"There are \n\nmultiple possible status codes that a request will return. For the purpose of simplicity, the two most important codes are:\n\n200 OK: The server was able to successfully process the request and return the requested data\n\n400 Bad Request: The server was not able to process the request do to an invalid request (usally the result of an invalid URL or unknown parameters)\n\n\n\n","type":"content","url":"/notebooks/api-foundations/api-basics#understanding-status-codes","position":13},{"hierarchy":{"lvl1":"API Basics","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/api-foundations/api-basics#imports","position":14},{"hierarchy":{"lvl1":"API Basics","lvl2":"Imports"},"content":"\n\nimport requests # access API\nimport matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/api-foundations/api-basics#imports","position":15},{"hierarchy":{"lvl1":"API Basics","lvl3":"Requests","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/api-foundations/api-basics#requests","position":16},{"hierarchy":{"lvl1":"API Basics","lvl3":"Requests","lvl2":"Imports"},"content":"The \n\nRequests Python package is a library that manages the requests made in Python\n\nA request returns machine-readable data in a JSON. Among the data, requests returns:\n\nrequest.status_code: status code of the request (200, 400, etc...)\n\nrequest.text: data requested as a JSON\n\nrequest.json(): fully JSON returned by the request\n\n\n\n","type":"content","url":"/notebooks/api-foundations/api-basics#requests","position":17},{"hierarchy":{"lvl1":"API Basics","lvl2":"Example: Weather API"},"type":"lvl2","url":"/notebooks/api-foundations/api-basics#example-weather-api","position":18},{"hierarchy":{"lvl1":"API Basics","lvl2":"Example: Weather API"},"content":"\n\nThe \n\nNational Weather Serivce manages an \n\nAPI for weather in the United States. The API is hosted at https://api.weather.gov/\n\nThe first step when working with an API should be to check that the API is fuctioning by querying a general request without any additional parameters.\n\nweather_request = requests.get(\"https://api.weather.gov/\")\nweather_request.status_code\n\nInfo\n\nA 200 status code represents that the base API is working as expected and a query request returns no errors.\n\nHowever, without any additional parameters, all the data that the request returns is just the status code.\n\n# All Data\nweather_request.json()\n\nThe next step is to query with specific paramters. For the weather API the accepts either a grid around a NWS Weather Forecast Office or a specific latitude/longtiude position\n\nThe request will be formatted as:https://api.weather.gov/points/<latitude>,<longitude>\n\nMore information about the documentation can be found at \n\nNWS Weather API.\n\nFor example, the location of the \n\nNCAR Mesa Lab is 39.97777 degrees latitude and -105.274966 degrees longitude\n\nncar_weather = requests.get(\"https://api.weather.gov/points/39.97777,-105.274966\")\n\n# Check request returned a valid response\nncar_weather.status_code\n\nSuccess\n\nWith a valid request and paramters, this request will return data as well!\n\nncar_weather.json()\n\nWith the latitude and longtiude of a position, the API will return information about the closest NWS forecast office that can be further queried to return the weather. A JSON acts like a Python dictionary; to return the values stored, json() can be queried for a specific key.\n\n# JSON as Dictionary\nprint(type(ncar_weather.json()))\nfor key, value in ncar_weather.json().items():\n    print(f\"\\nkey: {key}\")\n    print(f\"value: {value}\")\n\nThe closest forecast office from the NCAR Mesa Lab is forecastOffice\n\nncar_weather.json()[\"properties\"][\"forecastOffice\"]\n\nThe query also return the hourly forecast as a further URL to query as a request under forecastHourly\n\nncar_forecast_url = ncar_weather.json()[\"properties\"][\"forecastHourly\"]\nncar_forecast_url\n\nncar_forecast_hourly = requests.get(ncar_forecast_url)\nncar_forecast_hourly.status_code\n\nNote\n\nThere is a lot more data returned from this request! The forecast information can be collected under properties and period. Each period of time has various weather values to chose from:\n\nncar_forecast_hourly.json()[\"properties\"][\"periods\"][0].keys()\n\nTo plot, let’s collect the startTime, endTime and temperature (° F) values\n\ndatetime_start = ncar_forecast_hourly.json()[\"properties\"][\"periods\"][0][\"startTime\"]\ndatetime_end = ncar_forecast_hourly.json()[\"properties\"][\"periods\"][-1][\"endTime\"]\nprint(datetime_start)\nprint(datetime_end)\n\n# Temperatures every hour\nhour_x = []\ntemperature = []\nfor period in ncar_forecast_hourly.json()[\"properties\"][\"periods\"]:\n    hour_x.append(period[\"startTime\"])\n    temperature.append(period[\"temperature\"]) # collection of temperatures \nprint(temperature)\n\n# Plot\nfig, ax = plt.subplots(figsize=(22, 10))\n\n# Plot Hourly Temperature\nplt.bar(hour_x, temperature, color=\"dodgerblue\")\n\n# Setup Axis Limits and Title/Labels\nplt.title(f\"Hourly Tempearture at NCAR Mesa Lab for the Next {len(temperature)/24} Days\")\nplt.xlabel(\"Datetime\")\nplt.xticks(rotation=90, fontsize=8)\nplt.ylabel(\"Temperature (\\u00B0F)\")\nplt.show()\n\nSuccess\n\nWe have ploted the forecasted hourly temperatures for next week!\n\n\n\n","type":"content","url":"/notebooks/api-foundations/api-basics#example-weather-api","position":19},{"hierarchy":{"lvl1":"API Basics","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/api-foundations/api-basics#summary","position":20},{"hierarchy":{"lvl1":"API Basics","lvl2":"Summary"},"content":"In this notebook, we have...\n\ncovered basics about API terminology\n\ngone through a practice example of requesting data from the National Weather Service API\n\ngenerated a plot of forecasted temperatures","type":"content","url":"/notebooks/api-foundations/api-basics#summary","position":21},{"hierarchy":{"lvl1":"API Basics","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/api-foundations/api-basics#whats-next","position":22},{"hierarchy":{"lvl1":"API Basics","lvl3":"What’s next?","lvl2":"Summary"},"content":"Now we can continue investigating other APIs using some example workflows in this cookbook.\n\nCenter for Near-Earth Object Studies Fireball API\n\nEnvironmental Protection Agency’s Air Quality System API\n\nNational Aeronautics and Space Administration Earthaccess API\n\nWhiteface Mountain Cloud Water Request\n\n","type":"content","url":"/notebooks/api-foundations/api-basics#whats-next","position":23},{"hierarchy":{"lvl1":"API Basics","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/api-foundations/api-basics#resources-and-references","position":24},{"hierarchy":{"lvl1":"API Basics","lvl2":"Resources and references"},"content":"Python API Tutorial: Getting Started with APIs\n\nRequests Python Library\n\nNational Weather Service API","type":"content","url":"/notebooks/api-foundations/api-basics#resources-and-references","position":25},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API"},"type":"lvl1","url":"/notebooks/example-workflows/air-quality-system-api","position":0},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API"},"content":"\nPhoto by \n\nAhmer Kalam on \n\nUnsplash\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api","position":1},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API"},"type":"lvl1","url":"/notebooks/example-workflows/air-quality-system-api#visualizing-data-with-epas-air-quality-system-aqs-api","position":2},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API"},"content":"\n\n\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#visualizing-data-with-epas-air-quality-system-aqs-api","position":3},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/air-quality-system-api#overview","position":4},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Overview"},"content":"Air quality data are an important aspect of both atmospheric and environmental sciences. Understanding the concentrations of particulate matter and chemical species (e.g., O3 and NOx) can be useful for air pollution analysis from both the physical science and health science perspectives.\n\nThe US EPA AQS has archived data that have gone through quality assurance.\n\nIn this notebook, we will cover:\n\nAccessing data from the AQS\n\nExploring the format of the data\n\nPreparing the data for visualization\n\nGenerating a timeseries plot\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#overview","position":5},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/air-quality-system-api#prerequisites","position":6},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntroduction to Pandas\n\nNecessary\n\nHow to deal with dataframes and datasets\n\nMatplotlib Basics\n\nHelpful\n\nSkills for different plotting styles and techniques\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nEmail address for AQS API access\n\n\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#prerequisites","position":7},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/air-quality-system-api#imports","position":8},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Imports"},"content":"Info\n\nHere we'll import lots of stuff, but we might not end up using them all...\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom datetime import date\nfrom datetime import datetime\nimport numpy as np\nimport os\nimport pyaqsapi as aqs\n\nWe will also set some limits to the size of data that Pandas displays, so as not to overload our screens.\n\n# Set the maximum number of rows and columns to display\npd.set_option('display.max_rows', 10)  # Set to the number of rows you want to display\npd.set_option('display.max_columns', 10)  # Set to the number of columns you want to display\n\n\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#imports","position":9},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Accessing Data from the AQS"},"type":"lvl2","url":"/notebooks/example-workflows/air-quality-system-api#accessing-data-from-the-aqs","position":10},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Accessing Data from the AQS"},"content":"\n\nImportant:\n\nIf you have previously registered an account with the AQS, now will be a good time to get that information out and skip past the `aqs.aqs_sign_up()` step below.\n\nIf not, you should have an email address in mind that you'd like to use.\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#accessing-data-from-the-aqs","position":11},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Register a new email with aqs_sign_up()","lvl2":"Accessing Data from the AQS"},"type":"lvl3","url":"/notebooks/example-workflows/air-quality-system-api#register-a-new-email-with-aqs-sign-up","position":12},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Register a new email with aqs_sign_up()","lvl2":"Accessing Data from the AQS"},"content":"In the cell below, uncomment the code and replace ‘EMAIL’ with an email address to use for API credentials.\n\n# aqs.aqs_sign_up('EMAIL')\n\nIMPORTANT\n\nReplace your email address with 'EMAIL' after you've run `aqs_sign_up()`, or comment out the line!\n\nA new API key will be generated every time that line of code is executed!\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#register-a-new-email-with-aqs-sign-up","position":13},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl4":"Data can be pulled from the AQS in a number of different ways...","lvl3":"Register a new email with aqs_sign_up()","lvl2":"Accessing Data from the AQS"},"type":"lvl4","url":"/notebooks/example-workflows/air-quality-system-api#data-can-be-pulled-from-the-aqs-in-a-number-of-different-ways","position":14},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl4":"Data can be pulled from the AQS in a number of different ways...","lvl3":"Register a new email with aqs_sign_up()","lvl2":"Accessing Data from the AQS"},"content":"By Sample Site\n\nBy County\n\nBy State\n\nBy Lat/Lon Box\n\nBy Monitoring Agency\n\nBy Primary Quality Assurance Organization\n\nBy Core Based Statistical Aera (as defined by the US Census Bureau)\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#data-can-be-pulled-from-the-aqs-in-a-number-of-different-ways","position":15},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Let’s look at how the package deals with states...","lvl2":"Accessing Data from the AQS"},"type":"lvl3","url":"/notebooks/example-workflows/air-quality-system-api#lets-look-at-how-the-package-deals-with-states","position":16},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Let’s look at how the package deals with states...","lvl2":"Accessing Data from the AQS"},"content":"\n\n# aqs.aqs_states()\n\nWhoops!\n\nYou need to input your credentials before any of the functions will work!\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#lets-look-at-how-the-package-deals-with-states","position":17},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl5":"Use the aqs_credentials() function to input your username (email address) and access key.","lvl3":"Let’s look at how the package deals with states...","lvl2":"Accessing Data from the AQS"},"type":"lvl5","url":"/notebooks/example-workflows/air-quality-system-api#use-the-aqs-credentials-function-to-input-your-username-email-address-and-access-key","position":18},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl5":"Use the aqs_credentials() function to input your username (email address) and access key.","lvl3":"Let’s look at how the package deals with states...","lvl2":"Accessing Data from the AQS"},"content":"This is all found in the email you received when verifying your email address.\n\nIf you’ve previously registered your address and do not have the key, you can simply generate a new key by using the aqs_sign_up() funtion to resubmit your email address.\n\nLet’s also save our username and key as variables that we can easily call later.\n\nComment out the first line and Uncomment back in the second line in the cell below.\n\nReplace ‘AQS_USERNAME’ and ‘AQS_KEY’ with your credentials. We stored them as \n\nenvironment variables, to ensure they are kept secret while building this notebook.\n\naqs.aqs_credentials(username= os.getenv('AQS_USERNAME'), key= os.getenv('AQS_KEY'))\n#aqs.aqs_credentials(username='AQS_USERNAME', key='AQS_KEY')\n\n\nMAKE SURE TO CLEAR VARIABLES!Make sure to clear these variables before submitting a pull request! You do not want to share your credentials!\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#use-the-aqs-credentials-function-to-input-your-username-email-address-and-access-key","position":19},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Let’s look at those states now...","lvl2":"Accessing Data from the AQS"},"type":"lvl3","url":"/notebooks/example-workflows/air-quality-system-api#lets-look-at-those-states-now","position":20},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Let’s look at those states now...","lvl2":"Accessing Data from the AQS"},"content":"\n\naqs.aqs_states()\n\nSince states will be input via a number, let’s store aqs_states() as a variable that we can call on later to remind ourselves of what states we need.\n\nLet’s assume for now that we want to focus on New York and also save that code as variable.\n\nstates = aqs.aqs_states()\nNY = 36\n\nEverything Is Currently Input Numerically\n\nIt's important that we also address the fact that, currently, everything is input as a numerical value for pulling these data from the AQS with this Python package.\n\nParameter Codes can be accessed from the EPA \n\nhere, but to simplify things here are codes for a few common pollutants with defined Air Quality Index values you might be looking for...\n\nPollutant\n\nParameter Code\n\nCarbon Monoxide (CO)\n\n42101\n\nNitrogen Dioxide (NO2)\n\n42602\n\nOzone (O3)\n\n44201\n\nPM 10 (Total)\n\n81102\n\nOther variables that might be of interest:\n\nMET\n\nParameter Code\n\nWind Speed - Resultant (knots)\n\n61103\n\nWind Direction - Resultant (deg)\n\n61104\n\nOutdoor Temperature (F)\n\n62101\n\nAverage Ambient Temperature (C)\n\n68105\n\nRelative Humidity (%)\n\n62201\n\nBarometric Pressure (mbar)\n\n64101\n\nLet’s also store the parameter codes as variables to make things more simple.\n\nCO = 42101\nNO2 = 42602\nO3 = 44201\nPM10 = 81102\n\n\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#lets-look-at-those-states-now","position":21},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Exploring the format of the data"},"type":"lvl2","url":"/notebooks/example-workflows/air-quality-system-api#exploring-the-format-of-the-data","position":22},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Exploring the format of the data"},"content":"\n\nLet’s look at current O3 data for New York State.\n\nnow = datetime.today()\nyear = now.year\nmonth = now.month\nday = now.day\nprint(year, month, day)\n\nWarning\n\nThe AQS does not have real-time data. Also, note that the above times are in UTC!\n\nWe’ll subtract one day so we have the past day of data.\n\nozone = aqs.bystate.sampledata(parameter= O3, bdate = date(year=year, month=month, day = day-1), edate = date(year=year, month=month, day = day), stateFIPS=NY)\n\nozone\n\nOh, no!\n\nLooks like there isn't current O3 data available. We must go even further back in time.\n\nozone = aqs.bystate.sampledata(parameter= O3, bdate = date(year=year-1, month=month, day = day-1), edate = date(year=year-1, month=month, day = day), stateFIPS=NY)\n\n#ozone\n\nGreat! Now we have some data. Let’s look at the columns.\n\nozone.columns\n\nThere is a lot of information in this dataset.\n\nGeospatial Information\n\nTemporal Information\n\nSample Information\n\nData QA Information\n\nWe’ll focus on a few from 1, 2, and 3:\nLatitude and Longitude can be used to plot these data over a map, which will be addressed in Notebook 3 of this Cookbook\nLocal Date and Time , as well as State , County , and Site Number can be used as to isolate data for a time series.\nUnits of Measure will be necessary for annotations and labels.\n\nPAUSE\n\nWe've seen how to pull the data, and we've seen that pulling current data is not possible due to the lag between sample time and quality-assurance checks.\n\nYou are encouraged to check for more current data but, for now, let's look at this month's data from last year.\n\nstart = 1\nend = 30 #(Replace this value to match the appropriate \"last day\" of the month you are running this notebook)\n\nozone = aqs.bystate.sampledata(parameter= O3, bdate = date(year=year-1, month=month, day = start), edate = date(year=year-1, month=month, day = end), stateFIPS=NY)\n#ozone\n\nA quick check at plotting the data in its original format shows that some polishing is necessary.\n\nplt.plot(ozone['date_local'], ozone['sample_measurement'], '.')\nplt.show()\n\nSpatiotemporal Issues Abound!\n\nIt looks like the primary hiccups in trying to plot these data are the fact that the DataFrame has a separate column for date and time, and that there are multiple sample sites across the DataFrame.\n\nWe can combine the dates and times into a single datetime value.\n\nWe can also specify the sample sites we want to look at, or take averages across a county or the whole state.\n\nEither way, we'll need to prepare the data.\n\n\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#exploring-the-format-of-the-data","position":23},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Preparing the data for visualization"},"type":"lvl2","url":"/notebooks/example-workflows/air-quality-system-api#preparing-the-data-for-visualization","position":24},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Preparing the data for visualization"},"content":"Let’s utilize some Pandas features to generate a more manageable DataFrame for plotting.\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#preparing-the-data-for-visualization","position":25},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl5":"First, let’s select only one specific county--Albany","lvl2":"Preparing the data for visualization"},"type":"lvl5","url":"/notebooks/example-workflows/air-quality-system-api#first-lets-select-only-one-specific-county-albany","position":26},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl5":"First, let’s select only one specific county--Albany","lvl2":"Preparing the data for visualization"},"content":"Warning\n\nNot every county samples every type of pollutant!\n\nO3alb = ozone.loc[ozone['county'] == 'Albany', ['date_local', 'time_local', 'sample_measurement', 'units_of_measure', 'site_number', 'latitude', 'longitude']]\n\n#O3alb\n\nWarning\n\nSomething is a bit off, here. We see that the datetimes are mostly, but not entirely in order.\n\nOR... you see that there is no data at all. This is the result of data being sporatic over space and time\n\nLet’s make sure our new DataFrame for Albany is properly chronological.\n\nO3alb['datetime'] = pd.to_datetime(O3alb['date_local'] + ' ' + O3alb['time_local'])\nO3alb = O3alb.sort_values(by='datetime')\n\n#O3alb\n\nNow we should be able to plot a basic ozone time series for Albany, NY that covers this month for last year.\n\n\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#first-lets-select-only-one-specific-county-albany","position":27},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Generating a time series plot"},"type":"lvl2","url":"/notebooks/example-workflows/air-quality-system-api#generating-a-time-series-plot","position":28},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Generating a time series plot"},"content":"\n\nLet’s quickly test a lineplot of our data using seaborn.\n\nsns.lineplot(x=\"datetime\", y=\"sample_measurement\", data=O3alb)\nplt.show()\n\nSuccess\n\nWe have a time series! Now let's polish it up a bit...\n\n# Design the figure:\n\n# Figure shape\nfig, ax = plt.subplots(figsize = (10,5))\n\n# Give it a title\nplt.title((f'Ozone Concentration Albany, NY - {month}/{year-1}'), fontsize = 20)\n\n# Plot the data\nsns.lineplot(x=\"datetime\", y=\"sample_measurement\", data=O3alb, ax=ax)\n\n# Title the axes\nax.set_xlabel('Date', labelpad = 20, fontsize = 16)\nax.set_ylabel('[O$_{3}$] (ppm)', labelpad = 20, fontsize = 16)\n\n    # For the X-Axis\n\n# Set major x-ticks for midnight (00h)\nx_major = O3alb['datetime'][O3alb['datetime'].dt.hour == 0]\nax.set_xticks(x_major)\nprint(O3alb['datetime'].min())\nprint(O3alb['datetime'].max())\n# Set minor ticks at every 6 hours\nx_minor = pd.date_range(start= O3alb['datetime'].min(), end= O3alb['datetime'].max(), freq='6h')\nax.set_xticks(x_minor, minor=True)\n\n# Clean up the date label so it doesn't show the year or minutes\nformatted_labels = [x.strftime('%m-%d %H') + 'h' for x in x_major]\nax.set_xticklabels(formatted_labels, rotation=90, fontsize = 14)\n\n    # For the Y-Axis\n\n# Add minor ticks to the y-axis\ny_minor = np.arange(0, 0.06, 0.002)\nax.set_yticks(y_minor, minor = True)\n\n# Fix the fontsize for the y-tick labels\nax.tick_params(axis='y', labelsize=14)\n\nplt.show()\n\nWe can also add a new data points to compare (e.g., NO2)\n\nNO2_data = aqs.bystate.sampledata(parameter= NO2, bdate = date(year=year-1, month=month, day = start), edate = date(year=year-1, month=month, day = end), stateFIPS=NY)\n\nNO2_data['county'].unique()\n\nALBANY?!\n\nAlbany doesn't seem to display data for NO2.\n\nYou're welcome to check on your own for another pollutant to compare, but we'll shift to another county that covers both O3 and NO2.\n\nozone['county'].unique()\n\nThe Bronx seems like a good county to work with for the purposes of this Notebook.\n\nbronxNO2 = NO2_data.loc[NO2_data['county'] == 'Bronx', ['date_local', 'time_local', 'sample_measurement', 'units_of_measure', 'site_number', 'latitude', 'longitude']]\nbronxO3 = ozone.loc[ozone['county'] == 'Bronx', ['date_local', 'time_local', 'sample_measurement', 'units_of_measure', 'site_number', 'latitude', 'longitude']]\n\nprint('O3: ', bronxO3.iloc[0,:], '\\n')\nprint('NO2: ', bronxNO2.iloc[0,:])\n\nWe’ve seen that both data are present for Bronx County.\n\nAgain, let’s make sure our new DataFrames are properly chronological.\n\nbronxO3['datetime'] = pd.to_datetime(bronxO3['date_local'] + ' ' + bronxO3['time_local'])\nbronxO3 = bronxO3.sort_values(by='datetime')\n\nbronxNO2['datetime'] = pd.to_datetime(bronxNO2['date_local'] + ' ' + bronxNO2['time_local'])\nbronxNO2 = bronxNO2.sort_values(by='datetime')\n\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#generating-a-time-series-plot","position":29},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Now we’ll plot both data together.","lvl2":"Generating a time series plot"},"type":"lvl3","url":"/notebooks/example-workflows/air-quality-system-api#now-well-plot-both-data-together","position":30},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Now we’ll plot both data together.","lvl2":"Generating a time series plot"},"content":"\n\n# Design the figure:   \nfig, ax = plt.subplots(figsize = (10,5))\n\nshared_colors = sns.color_palette('muted')\n\n# Create a secondary y-axis\nax2 = ax.twinx()\n\n# Give it a title\nplt.title((f'Air Pollution Concentration Bronx County, NY - {month}/{year-1}'), fontsize = 20)\n\n# Plot the data for O3\nsns.lineplot(x=\"datetime\", y=\"sample_measurement\", data=bronxO3, label = 'O$_{3}$', ax = ax, legend = False, color= shared_colors[0],)\n\n# Plot NO2 data alongside O3 data\nsns.lineplot(x=\"datetime\", y=\"sample_measurement\", data=bronxNO2, label='NO$_{2}$', ax = ax2, legend = False, color= shared_colors[1],)\n\n\n    # For the X-Axis:\n\n# Title the x-axis\nax.set_xlabel('Date', labelpad = 20, fontsize = 16)\n\n# Set major x-ticks for midnight (00h)\nx_major = bronxO3['datetime'][bronxO3['datetime'].dt.hour == 0]\nax.set_xticks(x_major)\n\n# Set minor ticks at every 6 hours\nx_minor = pd.date_range(start= bronxO3['datetime'].min(), end= bronxO3['datetime'].max(), freq='6h')\nax.set_xticks(x_minor, minor=True)\n\n# Clean up the date label so it doesn't show the year or minutes\nformatted_labels = [x.strftime('%m-%d %H') + 'h' for x in x_major]\nax.set_xticklabels(formatted_labels, rotation=90, fontsize = 14)\n\n    # For the Y-Axes:\n\n# Add titles to both y-axes\nax.set_ylabel('[O$_{3}$] (ppm)', labelpad = 25, fontsize = 16)\nax2.set_ylabel('[NO$_{2}$] (ppm)', labelpad=25, fontsize=16, rotation = -90)\n\n# Add minor ticks to the y-axis for O3\ny_minor = np.arange(0, 0.06, 0.002)\nax.set_yticks(y_minor, minor = True)\n\n# Fix the fontsize for the y-tick labels\nax.tick_params(axis='y', labelsize=14)\n\n# Add minor ticks to the secondary y-axis for NO2\ny2_minor = np.arange(0, 55, 2)\nax2.set_yticks(y2_minor, minor=True)\n\n# Match fontsize for secondary y-axis\nax2.tick_params(axis='y', labelsize=14)\n\n    # Design the legend:\n\n# Combine both plot labels into a single legend\nhandles, labels = ax.get_legend_handles_labels()\nhandles2, labels2 = ax2.get_legend_handles_labels()\n\nplt.legend(handles + handles2, labels + labels2, loc='best', fontsize=18)\n\n\nplt.show()\n\nSuccess\n\nWe've been able to pull data from the AQS and plot a time series!\n\n\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#now-well-plot-both-data-together","position":31},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Let’s take a quick look at how you can pull data based on a areal extent, and then we’ll close out this notebook.","lvl2":"Generating a time series plot"},"type":"lvl3","url":"/notebooks/example-workflows/air-quality-system-api#lets-take-a-quick-look-at-how-you-can-pull-data-based-on-a-areal-extent-and-then-well-close-out-this-notebook","position":32},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl3":"Let’s take a quick look at how you can pull data based on a areal extent, and then we’ll close out this notebook.","lvl2":"Generating a time series plot"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#lets-take-a-quick-look-at-how-you-can-pull-data-based-on-a-areal-extent-and-then-well-close-out-this-notebook","position":33},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl4":"Let’s start with the CONUS","lvl3":"Let’s take a quick look at how you can pull data based on a areal extent, and then we’ll close out this notebook.","lvl2":"Generating a time series plot"},"type":"lvl4","url":"/notebooks/example-workflows/air-quality-system-api#lets-start-with-the-conus","position":34},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl4":"Let’s start with the CONUS","lvl3":"Let’s take a quick look at how you can pull data based on a areal extent, and then we’ll close out this notebook.","lvl2":"Generating a time series plot"},"content":"\n\nlonW = '-130'\nlonE = '-62'\nlatS = '20'\nlatN = '55'\n\nCONUS = aqs.bybox.sampledata(parameter= O3, bdate = date(year=year-1, month=month, day = day-1), \n                           edate = date(year=year-1, month=month, day = day), \n                           minlat = latS, maxlat = latN, minlon = lonW, maxlon = lonE)\n#CONUS\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#lets-start-with-the-conus","position":35},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl4":"We can also use the extent parameters for New York State.","lvl3":"Let’s take a quick look at how you can pull data based on a areal extent, and then we’ll close out this notebook.","lvl2":"Generating a time series plot"},"type":"lvl4","url":"/notebooks/example-workflows/air-quality-system-api#we-can-also-use-the-extent-parameters-for-new-york-state","position":36},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl4":"We can also use the extent parameters for New York State.","lvl3":"Let’s take a quick look at how you can pull data based on a areal extent, and then we’ll close out this notebook.","lvl2":"Generating a time series plot"},"content":"\n\nNY_E = -71\nNY_W = -81.0\nNY_N = 46\nNY_S = 40\n\nNYS = aqs.bybox.sampledata(parameter= O3, bdate = date(year=year-1, month=month, day = day-1), \n                           edate = date(year=year-1, month=month, day = day), \n                           minlat = NY_S, maxlat = NY_N, minlon = NY_W, maxlon = NY_E)\n#NYS\n\nSuccess\n\nWe've successfully fetched data from the AQS by areal extent for both the CONUS and for NYS!\n\nNOTE\n\nIf you want to only use data for within the state, be sure to filter the new DataFrame to ommit data points from other states that cross into the boundaries of your areal extent.\n\n\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#we-can-also-use-the-extent-parameters-for-new-york-state","position":37},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/air-quality-system-api#summary","position":38},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Summary"},"content":"In this notebook, we’ve\n\nmanaged to access air pollution data from the EPA’s AQS\n\nlooked at different ways to fetch the data\n\nlooked at different types of data available\n\nprepared the data for plotting\n\ngenerated time series plots of air pollution data\n\n(... include plotting over a map, direct user to other cookbooks for ideas on interactive visuals?)\n\nYou are encouraged to explore other variables within the dataset, and to utilize pandas and numpy functions to look at ways to manipulate and analyze these data!\n\n","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#summary","position":39},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/example-workflows/air-quality-system-api#resources-and-references","position":40},{"hierarchy":{"lvl1":"Visualizing Data with EPA’s Air Quality System (AQS) API","lvl2":"Resources and references"},"content":"Documentation for pyaqsapi: \n\nhttps://​usepa​.github​.io​/pyaqsapi​/pyaqsapi​.html\n\nMore information about the pyaqsapi package (developed by \n\nClinton McCrowey, EPA Region 3) can be found on GitHub: \n\nhttps://​github​.com​/USEPA​/pyaqsapi\n\nThe EPA’s AQS has general information and documentation here: \n\nhttps://​www​.epa​.gov​/aqs\n\nDetails about the specific parameter codes can be found here: \n\nhttps://​aqs​.epa​.gov​/aqsweb​/documents​/codetables​/parameters​.html\n\nTo access real-time data for air pollution, the \n\nAirNow API can be utilized.\n\nThanks to Daniel Garver (EPA Region 4) for help locating the AQS API, and for directing the authors of this Cookbook to the appropriate resources.\n\n\n\nInformation about the author: \n\nAdam Deitsch","type":"content","url":"/notebooks/example-workflows/air-quality-system-api#resources-and-references","position":41},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts"},"type":"lvl1","url":"/notebooks/example-workflows/cneos-fireball","position":0},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts"},"content":"\n\nBay Area Fireball, October 17, 2012\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball","position":1},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts"},"type":"lvl1","url":"/notebooks/example-workflows/cneos-fireball#nasa-api-world-map-of-fireball-impacts","position":2},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts"},"content":"\n\n\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#nasa-api-world-map-of-fireball-impacts","position":3},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/cneos-fireball#overview","position":4},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Overview"},"content":"This notebook will cover all the steps to access bright meteor and fireball impact data collected by NASA’s CNEOS and produce a global plot of impact sites\n\nPrerequisites\n\nCNEOS Overview\n\nImports\n\nAccess and Organize Data\n\nPlot Earth Fireball Impacts\n\nSummary\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#overview","position":5},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/cneos-fireball#prerequisites","position":6},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Matplotlib\n\nNecessary\n\nPlotting on a data\n\nIntro to GeoPandas\n\nNecessary\n\nPlotting on a world map\n\nIntro to Pandas\n\nNecessary\n\nFamiliarity with working with dataframes\n\n\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#prerequisites","position":7},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"CNEOS Overivew"},"type":"lvl2","url":"/notebooks/example-workflows/cneos-fireball#cneos-overivew","position":8},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"CNEOS Overivew"},"content":"\n\nA shooting star is a common term for a meteor and form bright trails of light that are often bright enough to be visible during the day. A “fireball” is a term for expectionally bright meteor that enters the Earth’s atmosphere at high speeds. A meteor that form fireballs can be over one meter long. A “bolide” typically refers to a fireball that breaks up in the atmosphere. These objects are tracked by the \n\nCenter for Near Earth Object Studies (CNEOS) and information about the Peak Brightness, Velocity, and Joules of energy radiatated can be retrieved from CNEOS via an API\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#cneos-overivew","position":9},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Fireball Data API","lvl2":"CNEOS Overivew"},"type":"lvl3","url":"/notebooks/example-workflows/cneos-fireball#fireball-data-api","position":10},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Fireball Data API","lvl2":"CNEOS Overivew"},"content":"\n\nThis notebook will query the Fireball API for data from the last decade of observations\n\nJPL’s Center for Near-Earth Objects Studies \n\nAPI “Fireball” is an API that will return machine-readable data in the form of a JSON.\n\nAll queries to the Fireball Data API make requests to https://ssd-api.jpl.nasa.gov/fireball.api\n\nThe API accepts different query parameters to filter the data\n\nDirectly querying https://ssd-api.jpl.nasa.gov/fireball.api will return all the data of fireball impacts in reverse-chronological order\n\nParameter\n\nType\n\nDescription\n\ndate-min\n\nstring\n\nexclude data earlier than this date YYYY-MM-DD or date/time YYYY-MM-DDThh:mm:ss\n\ndate-max\n\nstring\n\nexclude data later than this date YYYY-MM-DD or date/time YYYY-MM-DDThh:mm:ss\n\nenergy-min\n\nstring\n\nexclude data with total-radiated-energy less than this positive value in joules ×1010 (e.g., 0.3 = 0.3×1010 joules)\n\nenergy-max\n\nstring\n\nexclude data with total-radiated-energy greater than this (see energy-min)\n\nimpact-e-min\n\nstring\n\nexclude data with estimated impact energy less than this positive value in kilotons (kt) (e.g., 0.08 kt)\n\nimpact-e-max\n\nstring\n\nexclude data with total-radiated-energy greater than this (see impact-e-min)\n\nalt-min\n\nnumber\n\nexclude data from objects with an altitude less than this (e.g., 22 meaning objects smaller than this)\n\nalt-max\n\nnumber\n\nexclude data from objects with an altitude greater than this (e.g., 17.75 meaning objects larger than this)\n\nreq-loc\n\nboolean\n\nlocation (latitude and longitude) required; when set true, exclude data without a location\n\nreq-alt\n\nboolean\n\naltitude required; when set true, exclude data without an altitude\n\nreq-vel-comp\n\nboolean\n\npre-entry velocity components required; when set true, exclude data without pre-entry velocity components\n\nvel-comp\n\nboolean\n\ninclude pre-entry velocity components\n\nsort\n\nstring\n\nsort data on the specified field: “date”, “energy”, “impact-e”, “vel”, or “alt” (default sort order is ascending: prepend “-“ for descending)\n\nlimit\n\nnumber\n\nlimit data to the first N results (where N is the specified number and must be an integer value greater than zero)\n\nResults from API will be returned as JSON with a number of fields\n\nField\n\nDescription\n\ndate\n\ndate/time of peak brightness (GMT)\n\nlat\n\nlatitude at peak brightness (degrees)\n\nlon\n\nlongitude at peak brightness (degrees)\n\nlat-dir\n\nlatitude direction (“N” or “S”)\n\nlon-dir\n\nlatitude direction (“E” or “W”)\n\nalt\n\naltitude above the geoid at peak brightness (km)\n\nenergy\n\napproximate total radiated energy (10^10 joules)\n\nimpact-e\n\napproximate total impact energy (kt)\n\nvx\n\npre-entry velocity (Earth centered X component, km/s)\n\nvy\n\npre-entry velocity (Earth centered Y component, km/s)\n\nvz\n\npre-entry velocity (Earth centered Z component, km/s)","type":"content","url":"/notebooks/example-workflows/cneos-fireball#fireball-data-api","position":11},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Example JSON result"},"type":"lvl2","url":"/notebooks/example-workflows/cneos-fireball#example-json-result","position":12},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Example JSON result"},"content":"Return the last 3 records https://ssd-api.jpl.nasa.gov/fireball.api?limit=3{\n    \"signature\":\n        {\n            \"version\":\"1.0\",\n            \"source\":\"NASA/JPL Fireball Data API\"\n        },\n    \"count\":\"3\",\n    \"fields\":[\"date\",\"energy\",\"impact-e\",\"lat\",\"lat-dir\",\"lon\",\"lon-dir\",\"alt\",\"vel\"],\n    \"data\":[\n        [\"2024-06-03 01:13:51\",\"2.6\",\"0.092\",\"63.1\",\"S\",\"53.2\",\"W\",\"60.0\",null],\n        [\"2024-06-01 23:24:59\",\"7.9\",\"0.25\",\"1.0\",\"S\",\"15.9\",\"W\",\"26.5\",\"12.6\"],\n        [\"2024-05-27 03:19:36\",\"8.4\",\"0.26\",\"1.7\",\"N\",\"39.4\",\"W\",\"56.0\",\"39.2\"]\n    ]\n}\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#example-json-result","position":13},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/cneos-fireball#imports","position":14},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Imports"},"content":"\n\nimport requests                 # submit API request query\nimport pandas as pd             # organizes data retrieved by the API\nimport geopandas                # generate a world map\nimport matplotlib.pyplot as plt # plotting data\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#imports","position":15},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Request Fireball Data via API"},"type":"lvl2","url":"/notebooks/example-workflows/cneos-fireball#request-fireball-data-via-api","position":16},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Request Fireball Data via API"},"content":"\n\nTo retrieve all the fireballs recorded in the last decade (2014-2024)\n\ndata_since = \"2014-01-01\"\nlast_decade_fireball = requests.get(f\"https://ssd-api.jpl.nasa.gov/fireball.api?date-min={data_since}\")\nlast_decade_fireball.status_code # 200 is a valid request\n\nfireball_json = last_decade_fireball.json()\n\n# print out data labels\nprint(f\"fields = {fireball_json['fields']}\")\n\n# convert JSON data to a Pandas dataframe\nfireball_df = pd.DataFrame(fireball_json[\"data\"], columns=fireball_json[\"fields\"])\nfireball_df\n\n# remove nan and duplicated data rows and reindex rows\nfireball_df.dropna(inplace=True, ignore_index=True)\n\n# convert columnss from strings to a float\nfireball_df[\"energy\"] = fireball_df[\"energy\"].astype(float)\nfireball_df[\"impact-e\"] = fireball_df[\"impact-e\"].astype(float)\nfireball_df[\"lat\"] = fireball_df[\"lat\"].astype(float)\nfireball_df[\"lon\"] = fireball_df[\"lon\"].astype(float)\nfireball_df[\"alt\"] = fireball_df[\"alt\"].astype(float)\nfireball_df[\"vel\"] = fireball_df[\"vel\"].astype(float)\n\n# convert latitude to negative if lat-dir is S and longitude to negative if lon-dir is W\nfireball_df['lat'] = fireball_df['lat'].mask(fireball_df['lat-dir'].eq('S'), -fireball_df['lat'])\nfireball_df['lon'] = fireball_df['lon'].mask(fireball_df['lon-dir'].eq('W'), -fireball_df['lon'])\n\n# rename columns\nfireball_df = fireball_df.rename(columns= {\"impact-e\": \"impact energy\",\n                                           \"lat\": \"latitude\",\n                                           \"lat-dir\": \"latitude direction\",\n                                           \"lon\": \"longitude\",\n                                           \"lon-dir\": \"longitude direction\",\n                                           \"alt\": \"altitude\",\n                                           \"vel\": \"velocity\"})\n\n\nfireball_df\n\nfireball_df.info()\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#request-fireball-data-via-api","position":17},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Plot the Energy of Fireballs","lvl2":"Request Fireball Data via API"},"type":"lvl3","url":"/notebooks/example-workflows/cneos-fireball#plot-the-energy-of-fireballs","position":18},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Plot the Energy of Fireballs","lvl2":"Request Fireball Data via API"},"content":"\n\n# retrieve world map from GeoPandas\nworld_map = geopandas.read_file(geopandas.datasets.get_path(\"naturalearth_lowres\"))\n\n# Set up world map plot\nfig, ax = plt.subplots(figsize=(15, 10))\nworld_map.plot(color=\"grey\", ax=ax)\n\n# Plot Fireball Locations with Energy\nx = fireball_df[\"longitude\"]\ny = fireball_df[\"latitude\"]\nz = fireball_df[\"energy\"]\nplt.scatter(x, y, s=5*z, c=z, cmap=\"autumn\")\nplt.colorbar(label=\"Approximate Total Radiated Energy (10^10 Joules)\")\n\n# Setup Axis Limits and Title/Labels\nplt.xlim([-180, 180])\nplt.ylim([-90, 90])\nplt.title(\"Energy of Fireball Observations\")\nplt.xlabel(\"Longitude (Degrees)\")\nplt.ylabel(\"Latitude (Degrees)\")\nplt.show()\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#plot-the-energy-of-fireballs","position":19},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Plot the Impact Energy of Fireballs","lvl2":"Request Fireball Data via API"},"type":"lvl3","url":"/notebooks/example-workflows/cneos-fireball#plot-the-impact-energy-of-fireballs","position":20},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Plot the Impact Energy of Fireballs","lvl2":"Request Fireball Data via API"},"content":"\n\n# retrieve world map from GeoPandas\nworld_map = geopandas.read_file(geopandas.datasets.get_path(\"naturalearth_lowres\"))\n\n# Set up world map plot\nfig, ax = plt.subplots(figsize=(15, 10))\nworld_map.plot(color=\"grey\", ax=ax)\n\n# Plot Fireball Locations with Energy\nx = fireball_df[\"longitude\"]\ny = fireball_df[\"latitude\"]\nz = fireball_df[\"impact energy\"]\nplt.scatter(x, y, s=5*z, c=z, cmap=\"autumn\")\nplt.colorbar(label=\"Approximate Total Impact Energy (kt)\")\n\n# Setup Axis Limits and Title/Labels\nplt.xlim([-180, 180])\nplt.ylim([-90, 90])\nplt.title(\"Total Impact Energy of Fireball Observations\")\nplt.xlabel(\"Longitude (Degrees)\")\nplt.ylabel(\"Latitude (Degrees)\")\nplt.show()\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#plot-the-impact-energy-of-fireballs","position":21},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Plot the Velocity of Fireballs","lvl2":"Request Fireball Data via API"},"type":"lvl3","url":"/notebooks/example-workflows/cneos-fireball#plot-the-velocity-of-fireballs","position":22},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Plot the Velocity of Fireballs","lvl2":"Request Fireball Data via API"},"content":"\n\n# retrieve world map from GeoPandas\nworld_map = geopandas.read_file(geopandas.datasets.get_path(\"naturalearth_lowres\"))\n\n# Set up world map plot\nfig, ax = plt.subplots(figsize=(15, 10))\nworld_map.plot(color=\"grey\", ax=ax)\n\n# Plot Fireball Locations with Velocity\nx = fireball_df[\"longitude\"]\ny = fireball_df[\"latitude\"]\nz = fireball_df[\"velocity\"]\nplt.scatter(x, y, s=5*z, c=z, cmap=\"winter\")\nplt.colorbar(label=\"Pre-Entry Velocity (km/s)\")\n\n# Setup Axis Limits and Title/Labels\nplt.xlim([-180, 180])\nplt.ylim([-90, 90])\nplt.title(\"Pre-Entry Velocity of Fireball Observations\")\nplt.xlabel(\"Longitude (Degrees)\")\nplt.ylabel(\"Latitude (Degrees)\")\nplt.show()\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#plot-the-velocity-of-fireballs","position":23},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Plot the Altitude of Fireballs","lvl2":"Request Fireball Data via API"},"type":"lvl3","url":"/notebooks/example-workflows/cneos-fireball#plot-the-altitude-of-fireballs","position":24},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"Plot the Altitude of Fireballs","lvl2":"Request Fireball Data via API"},"content":"\n\n# retrieve world map from GeoPandas\nworld_map = geopandas.read_file(geopandas.datasets.get_path(\"naturalearth_lowres\"))\n\n# Set up world map plot\nfig, ax = plt.subplots(figsize=(15, 10))\nworld_map.plot(color=\"grey\", ax=ax)\n\n# Plot Fireball Locations with Energy\nx = fireball_df[\"longitude\"]\ny = fireball_df[\"latitude\"]\nz = fireball_df[\"altitude\"]\nplt.scatter(x, y, s=5*z, c=z, cmap=\"bone\")\nplt.colorbar(label=\"Altitude Above Geoid at Peak Brightness (km)\")\n\n# Setup Axis Limits and Title/Labels\nplt.xlim([-180, 180])\nplt.ylim([-90, 90])\nplt.title(\"Altitude of Fireball Observations\")\nplt.xlabel(\"Longitude (Degrees)\")\nplt.ylabel(\"Latitude (Degrees)\")\nplt.show()\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#plot-the-altitude-of-fireballs","position":25},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Last Section"},"type":"lvl2","url":"/notebooks/example-workflows/cneos-fireball#last-section","position":26},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Last Section"},"content":"If you’re comfortable, and as we briefly used for our embedded logo up top, you can embed raw html into Jupyter Markdown cells (edit to see):\n\nInfoYour relevant information here!\n\nFeel free to copy this around and edit or play around with yourself. Some other admonitions you can put in:\n\nSuccessWe got this done after all!\n\nWarningBe careful!\n\nDangerScary stuff be here.\n\nWe also suggest checking out Jupyter Book’s \n\nbrief demonstration on adding cell tags to your cells in Jupyter Notebook, Lab, or manually. Using these cell tags can allow you to \n\ncustomize how your code content is displayed and even \n\ndemonstrate errors without altogether crashing our loyal army of machines!\n\n\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#last-section","position":27},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/cneos-fireball#summary","position":28},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Summary"},"content":"Add one final --- marking the end of your body of content, and then conclude with a brief single paragraph summarizing at a high level the key pieces that were learned and how they tied to your objectives. Look to reiterate what the most important takeaways were.","type":"content","url":"/notebooks/example-workflows/cneos-fireball#summary","position":29},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/example-workflows/cneos-fireball#whats-next","position":30},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl3":"What’s next?","lvl2":"Summary"},"content":"Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!\n\n","type":"content","url":"/notebooks/example-workflows/cneos-fireball#whats-next","position":31},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/example-workflows/cneos-fireball#resources-and-references","position":32},{"hierarchy":{"lvl1":"NASA API: World Map of Fireball Impacts","lvl2":"Resources and references"},"content":"More information about \n\nplotting a world map with GeoPandas","type":"content","url":"/notebooks/example-workflows/cneos-fireball#resources-and-references","position":33},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy"},"type":"lvl1","url":"/notebooks/example-workflows/earthaccess-sla-sss","position":0},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss","position":1},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy"},"type":"lvl1","url":"/notebooks/example-workflows/earthaccess-sla-sss#data-access-via-earthaccess-library-and-vizualization-with-cartopy","position":2},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy"},"content":" In this Notebook, we will access data via the earthaccess python library. Then, we will do some quick visualization to find out the data of interest, to then make informational plots and, if desired, download data.  \n\n\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#data-access-via-earthaccess-library-and-vizualization-with-cartopy","position":3},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/earthaccess-sla-sss#overview","position":4},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Overview"},"content":"Within this notebook, we will cover:\n\nHow to search data via Earthdata Search web application\n\nHow to access NASA Earth Science data via earthaccess\n\nHow to subset, set attributes and modify coordinates for xarray Datasets\n\nHow to visualize data with hvplot and cartopy\n\nHow to download data Downloading data via Graphical User Interfaces (GUI) can be great for small datasets in isolated instances, but it has a few disadvantages, such as\n* it can be tedious - if you have to get data multiple times, repeatedly doing the same task is not a lot of fun, and this time could be better spent\n* it is more prone to human error - if you do many times the same task, odds are at some point you will make some mistake\n* storage issues - Sometimes it is hard to subset the data via GUI and you're stuck downloading large files when you needed just a small portion of them\n* it is not best science reproducibility practice - you can have the source of the data in your documents, but if you can have the actual data access method within the code, that makes it much easier for the work to be reproduced and for other researchers to build upon it\n\nAccessing data programatically, via application programming interface (API), is pretty advantageous when you consider those issues. In this notebook we will present an example of a workflow with access to data via `earthaccess`, a python library for accessing NASA Earth Science data.  \n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#overview","position":5},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/earthaccess-sla-sss#prerequisites","position":6},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Prerequisites"},"content":" In this notebook, we will access some data with the `xarray` library and plot some maps with the `matplotlib` and `cartopy` python libraries. \n\nConcepts\n\nImportance\n\nNotes\n\nXarray\n\nNecessary\n\nData and metadata structure\n\nnetCDF\n\nHelpful\n\nData and metadata structure\n\nUnderstanding of matplotlib\n\nHelpful\n\nFamiliarity with plots\n\nIntro to Cartopy\n\nHelpful\n\nFamiliarity with maps\n\nTime to learn: 30 minutes\n\n\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#prerequisites","position":7},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/earthaccess-sla-sss#imports","position":8},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Imports"},"content":"\n\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\n\nimport earthaccess \nimport xarray as xr\nimport hvplot.xarray\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom cartopy import crs as ccrs, feature as cfeature\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#imports","position":9},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Sea surface height anomaly"},"type":"lvl2","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-height-anomaly","position":10},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Sea surface height anomaly"},"content":"\n\nLet’s look for some sea surface height anomaly data for the Western Tropical Atlantic. In order to access it, we will use the earthaccess python library. It is used to make it easier for the user to find, stream and download NASA Earth Science data. More information about this library can be found in their \n\ndocumentation page and \n\nGithub repository.\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-height-anomaly","position":11},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Registering for an Earthdata account and authenticating","lvl2":"Sea surface height anomaly"},"type":"lvl3","url":"/notebooks/example-workflows/earthaccess-sla-sss#registering-for-an-earthdata-account-and-authenticating","position":12},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Registering for an Earthdata account and authenticating","lvl2":"Sea surface height anomaly"},"content":"Before searching for data, it is necessary to register for an Earthdata login profile, which can be done easily and quickly \n\nthis way.\n\nAfter registering, earthaccess has to authenticate you as a user. There are a few ways to do that, as stated \n\nhere. The easiest way of doing is just executing auth = earthaccess.login() in your jupyter notebook, and that would prompt for input of the username and password for the user.\n\nFor the purposes of this demonstration we have environment variables that will be used for authentication. So you just need to execute the cell below:\n\nauth = earthaccess.login(strategy=\"environmnent\")\n\nAfter it is authenticated, we are ready to start our search for data!\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#registering-for-an-earthdata-account-and-authenticating","position":13},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Searching and accessing the data","lvl2":"Sea surface height anomaly"},"type":"lvl3","url":"/notebooks/example-workflows/earthaccess-sla-sss#searching-and-accessing-the-data","position":14},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Searching and accessing the data","lvl2":"Sea surface height anomaly"},"content":"earthaccess allows us to look for datasets (called DataCollections) and specific data files (called DataGranules). To look for them, we need some criteria in order to perform the search. Here we will use\n\na shortname, which is a dataset identifier;\n\na temporal window: we want data between those dates; and\n\na bounding box: we want data within that area.\n\nOne good way to get a better understanding of what to look for is visiting the \n\nEarthdata Search website. There you can search by keywords and select filters to see which data could be helpful in your research. Once you find a dataset (Collection) that interests you, you click on its name. Below we show an example in which the “MEaSUREs Gridded Sea Surface Height Anomalies Version 2205” is the dataset of interest.\n\n\n\nAfter clicking on the dataset name, you click on Collection Details, which will send you to a page that has more information about the dataset.\n\n\n\nFeel free to read the details about the dataset, to make sure that’s what would help you in your work. If so, copy the name, usually all in caps, that is shown inside a light gray box on the top of the page. That is the identifier for that specific dataset. That’s the main information we’ll need to provide earthaccess library to search for it.\n\n\n\nNow that we have an identifier for the dataset we want to access, let’s use earthaccess to find sea surface height anomaly data in September and October, 2020, in the Western Tropical Atlantic:\n\n# specify the bounding box for the Western Tropical Atlantic\nlonmin, latmin, lonmax, latmax = -70, -5, -45, 20\n\n# https://search.earthdata.nasa.gov/search/granules/collection-details?p=C2270392799-POCLOUD&pg%5B0%5D%5Bv%5D=f&pg%5B0%5D%5Bgsk%5D=-start_date&as%5Bscience_keywords%5D%5B0%5D=Oceans%3ASea%20Surface%20Topography%3ASea%20Level%3ASea%20Level%20Anomaly&tl=1718059227%213%21%21&fsm0=Sea%20Surface%20Topography&fst0=Oceans&fst1=Oceans&fsm1=Sea%20Surface%20Topography&fs11=Sea%20Level&fs21=Sea%20Level%20Anomaly&fpb0=Space-based%20Platforms&long=-0.0703125\nsla_shortname = \"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\"\n\nsla_results = earthaccess.search_data(\n    short_name=sla_shortname,\n    cloud_hosted=True,\n    temporal=(\"2020-09-01\", \"2020-10-30\"),\n    bounding_box=(lonmin, latmin, lonmax, latmax)\n)\n\nInfoNote: not all these arguments are necessary, but the more arguments you use, the more refined the search will be. More info \n\nhere.\n\nThe function search_data returns a list with the Granules found. Here we can think of Granules as files, or time-steps. Great, we found some that match our criteria. Now, let’s take a look at one of them:\n\ndict(sla_results[0])\n\nSo here we have all the metadata information about this first Granule. You can also check all the Granules from the list, if you’d like to make sure they are all of interest to you:\n\nsla_results \n\nOK, we found some results that interest us. Now let’s access them!\n\nTwo possibly important limiting factors for research are limited computing capacity and limited data storage. Even if data storage is not very limited, unecessarily working with large files is not best practice. So here we’ll not download the data; we’ll stream it instead, using \n\nxarray. Through xarray we can take a look at the data, subset, perform some analyses and, only if we are certain that we want to look further into it, download the data.\n\nSo let’s start by opening the data Granules as a dataset with xarray. This can take some time, depending on your system.\n\n%%time \nsla_ds = xr.open_mfdataset(earthaccess.open(sla_results))\nsla_ds\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#searching-and-accessing-the-data","position":15},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Subsetting the data, adjusting the coordinates and assigning attributes","lvl2":"Sea surface height anomaly"},"type":"lvl3","url":"/notebooks/example-workflows/earthaccess-sla-sss#subsetting-the-data-adjusting-the-coordinates-and-assigning-attributes","position":16},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Subsetting the data, adjusting the coordinates and assigning attributes","lvl2":"Sea surface height anomaly"},"content":"\n\nIf we take a look at the loaded dataset, we notice a few things:\n\nThe search yielded 12 granules, so we have 12 time-steps in the resulting dataset\n\nThe Latitude and Longitude span a much greater area than requested in the search. The dataset loaded contains the area that we requested, plus much more.\n\nwe have a few data variables besides SLA (Sea Level Anomaly Estimate)\n\nthe Longitude coordinate goes from 0 to 360 instead of from -180 to 180.\n\nthere are multiple attributes. Those are very important, since they contain dataset metadata information.\n\nThat being said, we need to do a few things to make this dataset ready for analysis, visualization, and/or download.\n\nThe first thing to do is to adjust the Longitude coordinates and sort the data accordingly:\n\n# convert Longitude from 0-360 to -180-180\nsla_ds.coords['Longitude'] = (sla_ds.coords['Longitude'] + 180) % 360 - 180\nsla_ds = sla_ds.sortby(sla_ds.Longitude)\nsla_ds\n\nNow we can see that the dataset has Longitude values between -180 and 180. Great!\n\nThe next step is to subset the data - we don’t need SLA for the entire globe, just for the Western Tropical Atlantic, so there’s no need to be messing with large files. To subset the data, we leverage the power of slicing in xarray:\n\nsla_subset = sla_ds['SLA'].sel(Latitude=slice(latmin, latmax), Longitude=slice(lonmin,lonmax))\nsla_subset\n\nThis looks better, but in order for this to be the main object of analysis and for us to be able to delete the original dataset (spanning the entire global ocean), we need to make sure it has metadata information. In order to do that, we gather some attributes from the original dataset, remove the ones that don’t make sense after subsetting, and assign these attributes to the sla_subset DataArray:\n\n# get attributes from the original Dataset\nsla_attrs = sla_ds.attrs\ndel sla_ds\n\n# remove some attributes\nattrs_to_be_removed = ['geospatial_lat_min', \n                       'geospatial_lat_max', \n                       'geospatial_lon_min', \n                       'geospatial_lon_max', \n                       'time_coverage_start', \n                       'time_coverage_end']\nfor attr in attrs_to_be_removed:\n    del sla_attrs[attr]\n    \n# assign attributes to DataArray    \nsla_subset = sla_subset.assign_attrs(sla_attrs)\n\n# check that the new attributes are there\nsla_subset.attrs\n\nGreat, now that we have data and metadata. Now, let’s take a quick look at the data for the few time-steps that we have loaded and see if they show any interesting features for us to investigate further. We can do that easily and interactively with hvplot:\n\nsla_subset.hvplot.image(x='Longitude', y='Latitude', aspect=\"equal\", cmap='RdBu_r', clim=(-0.4, 0.4), title=\"Sea Level Anomaly Estimate (m)\")\n\nAfter inspection, we decide that the data for Sept 23rd, 2020 looks promising. So let’s subset further, to have just this time-step in our DataArray:\n\ndate_selection = '2020-09-23' \nsla_subset_plot = sla_subset.sel(Time=date_selection)\ndel sla_subset\nsla_subset_plot\n\nNote that the subsetted DataArray has all the attributes that we have assigned from the original dataset.\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#subsetting-the-data-adjusting-the-coordinates-and-assigning-attributes","position":17},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Sea Surface height anomaly visualization","lvl2":"Sea surface height anomaly"},"type":"lvl3","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-height-anomaly-visualization","position":18},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Sea Surface height anomaly visualization","lvl2":"Sea surface height anomaly"},"content":"\n\nNow let’s make a more ellaborate visualization of the data to analyze the features better. For that, we will use the library cartopy.\n\n# initialize figure\nfig = plt.figure(figsize=(11, 8.5))\nax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n\n# add features to map\nax.coastlines()\nax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='black')\ngl = ax.gridlines(\n    draw_labels=True, linewidth=2, color='gray', alpha=0.5, linestyle='--'\n)\nax.set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())\n\n# plot data\nlevs = np.linspace(0,0.5,6)\nfmt = '%1.1f'\nsla_subset_plot.plot(vmin=-0.5, vmax=0.5, cmap ='RdBu_r', transform=ccrs.PlateCarree())\ncs = sla_subset_plot.squeeze().plot.contour(levels=levs,colors='k')\nax.clabel(cs, levs, fmt=fmt, inline=True, fontsize=10)\n\nThe colorbar centered in zero helps us understand the data better. The main features are in the eastern part, where close to the coast we can see strong gradients, but altimetry data too close to the coast might have some issues. So we focus on the features further from the coast, and the main ones we see are circular contour lines between 5°N and 10°N, and 45°W and 55°W. Some knowledge of the ocean circulation patterns in the area suggest that these features are associated with the North Brazil Current retroflection and a North Brazil Current Ring. Can we see any signature of that in salinity? Let’s check!\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-height-anomaly-visualization","position":19},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Sea surface salinity data"},"type":"lvl2","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-salinity-data","position":20},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Sea surface salinity data"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-salinity-data","position":21},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Searching and accessing the data","lvl2":"Sea surface salinity data"},"type":"lvl3","url":"/notebooks/example-workflows/earthaccess-sla-sss#searching-and-accessing-the-data-1","position":22},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Searching and accessing the data","lvl2":"Sea surface salinity data"},"content":"Similarly to what was done for sea surface height anomaly, we’ll visit the  \n\nEarthdata Search website to see what salinity data is available for the region and the time-period we’re interested. After some search, we find that the dataset with the shortname “SMAP_RSS_L3_SSS_SMI_8DAY-RUNNINGMEAN_V5” seems applicable, even though it’s an 8-day mean. Let’s set the same bounding box, the time span for around the same day we analyzed for sea surface height anomaly, and see what data is available:\n\n# https://search.earthdata.nasa.gov/search/granules/collection-details?p=C2208425700-POCLOUD&pg%5B0%5D%5Bv%5D=f&pg%5B0%5D%5Bqt%5D=2021-09-01%2C2021-11-30&pg%5B0%5D%5Bgsk%5D=-start_date&tl=1718241606.658%213%21%21\nsss_dataname=\"SMAP_RSS_L3_SSS_SMI_8DAY-RUNNINGMEAN_V5\"\n\nsss_results = earthaccess.search_data(\n    short_name=sss_dataname,\n    cloud_hosted=True,\n    temporal=(\"2020-09-23\",\"2020-09-24\"), # considering salt_results = salt_results[::3], the second time-step is the best for showing a ring and comparing to altimetry\n    bounding_box=(lonmin, latmin, lonmax, latmax)\n)\n\nWe notice that there were 9 granules for the time span between 2020-09-23 and 2020-09-24, which seems excessive. Let’s take a look at some of the granules:\n\nsss_results[0:3]\n\nLooking at the EndingDateTime and BeginningDateTime for each granule, we conclude that earthaccess found all the granules in which the date 2020-09-23 and/or 2020-09-24 were used to calculate the 8-day mean, and that’s why there are so many granules.\n\nLet’s load the dataset:\n\n%%time\nsss_ds = xr.open_mfdataset(earthaccess.open(sss_results))\nsss_ds\n\nIn this dataset we see many of the same issues we saw in the sea surface height anomaly dataset:\n\nlarge area, much bigger than the bouding box\n\nother data variables besides sea surface salinity\n\nlongitude between 0 and 360 instead of -180 and 180\n\nmultiple attributes - that’s a good thing!\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#searching-and-accessing-the-data-1","position":23},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Subsetting the data, adjusting the coordinates and assigning attributes","lvl2":"Sea surface salinity data"},"type":"lvl3","url":"/notebooks/example-workflows/earthaccess-sla-sss#subsetting-the-data-adjusting-the-coordinates-and-assigning-attributes-1","position":24},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Subsetting the data, adjusting the coordinates and assigning attributes","lvl2":"Sea surface salinity data"},"content":"Now we need to adjust this dataset, very similarly to what we did for the sea surface height anomaly one. First, we convert the longitude coordinates and sort the data accordingly:\n\nsss_ds.coords['lon'] = (sss_ds.coords['lon'] + 180) % 360 - 180\nsss_ds = sss_ds.sortby(sss_ds.lon)\nsss_ds\n\nThen we subset just for the variable we want, for the area within the bounding box, and make sure to assign most attributes from the original dataset:\n\n# geographically subset\nsss_subset = sss_ds['sss_smap_40km'].sel(lat=slice(latmin, latmax), lon=slice(lonmin,lonmax))\n\n# get attributes from original dataset\nsss_attrs = sss_ds.attrs\ndel sss_ds\n\n# remove attributes that don't apply\nattrs_to_be_removed = ['center_day_of_observation',\n                      'first_orbit',\n                      'last_orbit',\n                      'geospatial_lat_min', \n                      'geospatial_lat_max', \n                      'geospatial_lon_min', \n                      'geospatial_lon_max', \n                      'time_coverage_start', \n                      'time_coverage_end']\nfor attr in attrs_to_be_removed:\n    del sss_attrs[attr]\n    \nsss_subset = sss_subset.assign_attrs(sss_attrs)\nsss_subset\n\nNow we need to choose a time-step. If we look at the coordinates “time”, we’ll notice that the days in it are the center days of the running mean. So we’ll just pick the same day used for the sea surface height anomaly data, ‘2020-09-23’:\n\ndate_selection = '2020-09-23'\nsss_subset_plot = sss_subset.sel(time=date_selection)\ndel sss_subset\n\nsss_subset_plot\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#subsetting-the-data-adjusting-the-coordinates-and-assigning-attributes-1","position":25},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Sea surface salinity visualization","lvl2":"Sea surface salinity data"},"type":"lvl3","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-salinity-visualization","position":26},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"Sea surface salinity visualization","lvl2":"Sea surface salinity data"},"content":"\n\nGreat, we got a DataArray with just one time-step; now let’s plot a map of it:\n\n# initialize the figure\nfig = plt.figure(figsize=(23, 8.5))\n\n# add features to map\nax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax.coastlines()\nax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='black')\ngl = ax.gridlines(\n    draw_labels=True, linewidth=2, color='gray', alpha=0.5, linestyle='--'\n)\nax.set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())\n\n# plot data\nlevs_sss = np.linspace(5,37,12)\nfmt_sss = '%1.0f'\nsss_subset_plot.plot(vmin=5, vmax=37, transform=ccrs.PlateCarree())\ncs = sss_subset_plot.squeeze().plot.contour(levels=levs_sss,colors='k')\nax.clabel(cs, levs_sss, fmt=fmt_sss, inline=True, fontsize=10)\n\nThe sea surface salinity map seems good, but if we put the plots next to each other, we could have a better understanding of what’s going on.\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-salinity-visualization","position":27},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Sea surface height anomaly and sea surface salinity combined visualization"},"type":"lvl2","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-height-anomaly-and-sea-surface-salinity-combined-visualization","position":28},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Sea surface height anomaly and sea surface salinity combined visualization"},"content":"\n\n# initialize figure\nfig, (ax1, ax2) = plt.subplots(1,2,\n                               subplot_kw = {'projection':ccrs.PlateCarree()},\n                               figsize=(25, 8.5))\n# add features to subplots\nfor ax in (ax1, ax2):\n    ax.coastlines()\n    ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='black')\n    gl = ax.gridlines(\n        draw_labels=True, linewidth=2, color='gray', alpha=0.5, linestyle='--'\n    )\n    ax.set_extent([lonmin, lonmax, latmin, latmax], crs=ccrs.PlateCarree())\n\n# plot sla\nlevs_sla = np.linspace(0,0.5,6)\nfmt_sla = '%1.1f'\nsla_subset_plot.plot(ax= ax1,vmin=-0.5, vmax=0.5, cmap ='RdBu_r', transform=ccrs.PlateCarree())\ncs = sla_subset_plot.squeeze().plot.contour(ax= ax1,levels=levs_sla,colors='k')\nax1.clabel(cs, levs_sla, fmt=fmt_sla, inline=True, fontsize=10)\n\n# plot sss\nlevs_sss = np.linspace(5,37,10)\nfmt_sss = '%1.0f'\nsss_subset_plot.plot(ax= ax2,vmin=5, vmax=37, transform=ccrs.PlateCarree())\ncs = sss_subset_plot.squeeze().plot.contour(ax= ax2,levels=levs_sss,colors='k')\nax1.clabel(cs, levs_sss, fmt=fmt_sss, inline=True, fontsize=10)\nplt.savefig(\"subplots.png\",dpi=100)\n\nGreat; with the maps side-by-side, it’s easier to understand the features. In the salinity map, we can see the freshwater discharge from the Orinoco and Amazon Rivers, close to the coast, at about 61°W,9°N and -48°W,0°, respectively. In addition, we can see a somewhat circular fresher water feature between about 50°W-55°W and 5°N-10°N. The location of this feature coincides with the location of the high circular sea surface height anomaly contours, strenghtening our hypothesis that it’s a North Brazil Current Ring transporting freshwater from the Amazon River to the Caribbean. To the right, one can see the higher salinity values in a concave configuration, coinciding with the location of other circular sea surface height anomaly contours, suggesting this is a signature of the North Brazil Current retroflection, bringing salty water from the Equatorial Atlantic. For more information on this process, see \n\nthis article.\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#sea-surface-height-anomaly-and-sea-surface-salinity-combined-visualization","position":29},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Saving the data"},"type":"lvl2","url":"/notebooks/example-workflows/earthaccess-sla-sss#saving-the-data","position":30},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Saving the data"},"content":"Remember: we have done all this visualization without downloading one single data file! That’s one of the best functionalities of xarray, the ability of looking at the data with “no strings attached”, so you can only download once you know that dataset will work for you.\n\nNow that we’ve seen that the subset DataArrays have interesting features, we may want to download them to our local machines, so we can analyze them further. That’s very easy to do with xarray, by saving it in netCDF format. Here we build some filenames to use to save the DataArrays, but of course we could use any filename.\n\nOne very important thing to notice is that the saved files will contain the attributes we assigned from the original Dataset. This is necessary for reproducibility, so other people can build on your work, or even future you can have access to details of the data.\n\nWarningHere we leave the saving commands commented, because if you continuously save files you can run out of storage. So be mindful when saving files!\n\nsss_filename = sss_subset_plot.attrs['standard_name'] + \".nc\"\nsla_filename = sla_subset_plot.attrs['standard_name'] + \".nc\"\n\n# saving files - be careful here so you won't run out of storage!\n# sss_subset_plot.to_netcdf(sss_filename)\n# sla_subset_plot.to_netcdf(sla_filename)\n\n\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#saving-the-data","position":31},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/earthaccess-sla-sss#summary","position":32},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Summary"},"content":"In this notebook we were able to leverage the earthaccess library to access large datasets with xarrray and visualize them with matplotlib and cartopy. We learned the advantages of accessing data programatically, including promoting science reproducibility. We were able to subset data and then, only when we identified data that would be of interest, download it. That is specially advantageous for cases with limited available storage or limited computing capacity.","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#summary","position":33},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/example-workflows/earthaccess-sla-sss#whats-next","position":34},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl3":"What’s next?","lvl2":"Summary"},"content":"Nasa Earthdata Search has various types of data, in different formats and from different platforms. The user is certainly encouraged to play around with other types of data, to become more comfortable with this tool.\n\nThe earthaccess library is a wrap around the NASA Earth Science APIs. With the knowledge from this notebook, the user may understand bettter the structure of metadata in APIs and feel more comfortable accessing data via APIs from other sources, such as \n\nNCEI and \n\nUSGS.\n\n","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#whats-next","position":35},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/example-workflows/earthaccess-sla-sss#resources-and-references","position":36},{"hierarchy":{"lvl1":"Data access via earthaccess library and vizualization with cartopy","lvl2":"Resources and references"},"content":"Earthdata Search web application\n\ninfo on earthaccess\n\nearthaccess Github repository\n\nearthaccess documentation page\n\nEarthdata cloud clinic\n\nUSGS Science Data Catalog API Documentation\n\nNCEI API user documentation\n\nIntro to cartopy\n\nMatplotlib basics\n\nnetCDF and CF: the basics\n\nIntroduction to Xarray\n\nArticle on North Brazil Current Rings","type":"content","url":"/notebooks/example-workflows/earthaccess-sla-sss#resources-and-references","position":37},{"hierarchy":{"lvl1":"Whiteface Mountain Cloud Water Data"},"type":"lvl1","url":"/notebooks/example-workflows/wfm-cloud-water","position":0},{"hierarchy":{"lvl1":"Whiteface Mountain Cloud Water Data"},"content":"\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water","position":1},{"hierarchy":{"lvl1":"Whiteface Mountain Cloud Water Data"},"type":"lvl1","url":"/notebooks/example-workflows/wfm-cloud-water#whiteface-mountain-cloud-water-data","position":2},{"hierarchy":{"lvl1":"Whiteface Mountain Cloud Water Data"},"content":"\n\n\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#whiteface-mountain-cloud-water-data","position":3},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC"},"type":"lvl1","url":"/notebooks/example-workflows/wfm-cloud-water#accessing-cloud-water-data-from-the-asrc","position":4},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC"},"content":"","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#accessing-cloud-water-data-from-the-asrc","position":5},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/example-workflows/wfm-cloud-water#overview","position":6},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Overview"},"content":"Cloud water data provide an insight into the chemical processing of gasses and particulates in the atmosphere. While this is not technically an API, this notebook will show how to access a niche dataset for cloud water chemistry, collected in-situ at Whiteface Mountain in Wilmington, NY. The sample site serves as a relative background for atmospheric chemistry within the region, as it is a remote, mountain-top observatory.\n\nThis notebook will cover\n\nRequesting data access\n\nCleaning and sorting through the data\n\nBasic cloud water chemistry analysis (Coming Soon)\n\nPlotting the data (Coming Soon)\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#overview","position":7},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/example-workflows/wfm-cloud-water#prerequisites","position":8},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntroduction to Pandas\n\nNecessary\n\nHow to deal with dataframes and datasets\n\nMatplotlib Basics\n\nHelpful\n\nSkills for different plotting styles and techniques\n\nTime to learn: 45 minutes\n\nSystem requirements:\n\nEmail Address for Data Access\n\n\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#prerequisites","position":9},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/example-workflows/wfm-cloud-water#imports","position":10},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Imports"},"content":"Info\n\nHere we'll import lots of stuff, but we might not end up using them all...\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom datetime import date\nfrom datetime import datetime\nimport numpy as np\n\nWe will also set some limits to the size of data that Pandas displays, so as not to overload our screens.\n\n# Set the maximum number of rows and columns to display\npd.set_option('display.max_rows', 10)  # Set to the number of rows you want to display\npd.set_option('display.max_columns', 10)  # Set to the number of columns you want to display\n\n\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#imports","position":11},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Accessing the Data"},"type":"lvl2","url":"/notebooks/example-workflows/wfm-cloud-water#accessing-the-data","position":12},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Accessing the Data"},"content":"\n\nCurrently, the data from the Whiteface Mountain summit are obtained and managed by the \n\nLance Research Laboratory. Available data includes, among others, chemical speciation within cloud water:\n\nAnions\n\nCations\n\nSulfate\n\nAmmonium\n\nNitrate\n\nSodium\n\nChloride\n\nCalcium\n\nFormate\n\nMagnesium\n\nAcetate\n\nPotassium\n\nOxalate\n\n\n\nSome Other Data\n\nTotal Organic Carbon\n\npH\n\nConductivity\n\nLiquid Water Content\n\nSample Volume\n\nSample Dump Date/Time\n\nNote:\n\nIn order to access the data, we don't need an API. We just need to fill out a simple `Google Form` at the following website:\n\nhttp://atmoschem.asrc.cestm.albany.edu/~cloudwater/pub/Data.htm\n\n\n\nOnce you are granted access, you can utilize recent and historical data spanning back to 1994. The data come in *.xlsx files, or as multiple *.xlsx files in a zip drive, depending on which dataset you collect.\n\nThis notebook uses 2022 Cloud Water Data (current as of June 18th, 2024) as an example. As the data files come with various sheets covering multiple angles of quality control, we will simplify this notebook with a *.csv file of the “valid” samples.\n\nThe full data file can be viewed in ../files/WFC.2022.Data.R2--6_18_24.xlsx.\n\n\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#accessing-the-data","position":13},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Reading the Data"},"type":"lvl2","url":"/notebooks/example-workflows/wfm-cloud-water#reading-the-data","position":14},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Reading the Data"},"content":"We will utilize the Pandas package to handle our reading in our data file. We will also preemptively use the ISO-8859-1 encoding to ensure symbols like ° and μ work.\n\ndf = pd.read_csv('../files/WFC.2022.Data.R2--6_18_24.csv', encoding = 'ISO-8859-1')\n\nLet’s look at our dataframe...\n\ndf\n\nAs we can see above, the data actually begin on the fifth line.\n\nLet’s take a closer look and notice that there are only 42 samples in this particular set...\n\ndf.iloc[4:50,:]\n\nIn the next cell, we will use Row 4 for our column headings, and slice the dataframe so it only shows our data. Cleaning up the data is helpful for preemptively halting any errors resulting from NaNs and empty cells.\n\ndf.columns = df.iloc[4]\ndf = df.iloc [5:48]\ndf\n\nSome brief details about the data format...\n\nThe LABNO values represent the Julian date, where the first two digits are year, and the next three are the day. The remaining two digits refer to internal identification regarding the collection bottles for same-day samples.\n\nThe cloud water at Whiteface Mountain is collected in bulk 12-hour samples, so the time the accumulated sample was \"dumped\" into a storage container is in the DUMP TIME column, and the duration of time in that 12-hour period where the summit was in-cloud is show in in the COLLECTION_HOURS column.\n\nLet’s look at all the columns that have data in them below...\n\nfor col in df.columns:\n    if not df[col].isna().all():\n        print(col)\n\nNow that we have our data in a manageable format, we can begin any analysis or visualizations we are interested in.\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#reading-the-data","position":15},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Analyzing the Data"},"type":"lvl2","url":"/notebooks/example-workflows/wfm-cloud-water#analyzing-the-data","position":16},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Analyzing the Data"},"content":"\n\nComing Soon!This section is still under development.\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#analyzing-the-data","position":17},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Plotting the Data"},"type":"lvl2","url":"/notebooks/example-workflows/wfm-cloud-water#plotting-the-data","position":18},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Plotting the Data"},"content":"\n\nComing Soon!This section is still under development.\n\n\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#plotting-the-data","position":19},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/example-workflows/wfm-cloud-water#summary","position":20},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Summary"},"content":"In this notebook, we’ve covered how to access cloud water chemistry data from the \n\nLance Research Laboratory at the University at Albany’s Atmospheric Sciences Research Center. We’ve looked at the data format, and ways to process and analyze the data. This is a niche dataset, updated regularly as cloud water is collected, processed, and analyzed each summer.\n\n","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#summary","position":21},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/example-workflows/wfm-cloud-water#resources-and-references","position":22},{"hierarchy":{"lvl1":"Accessing Cloud Water Data from the ASRC","lvl2":"Resources and references"},"content":"More information about the Whiteface Mountain Field Station: \n\nhttps://​whiteface​.asrc​.albany​.edu/\n\nMore information about the \n\nLance Research Laboratory: \n\nhttps://​research​.asrc​.albany​.edu​/facstaff​/lance​/index​.html\n\nMore information about the cloud water chemistry at Whiteface Mountain: \n\nhttps://​acp​.copernicus​.org​/articles​/23​/1619​/2023/\n\nInformation about the author: \n\nAdam Deitsch","type":"content","url":"/notebooks/example-workflows/wfm-cloud-water#resources-and-references","position":23},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1}]}